\chapter{Experimental Method Setups}
\label{chap:methodology}


Personalized recommendation is domain-specific, in other words algorithms that work in one setting might work poorly in others that might have different characteristics. \cite{}
In this Chapter, I will describe the datasets, data pre-processing, data transformation, as well as recommendation and re-ranking algorithms that were used as baseline to perform experiments on them throughout this dissertation. 
Additionally, experimental details such as hyper-parameter tuning, the choice of the model and the choice of the protected attribute(s) in each dataset will be explained thoroughly. For my experiments, I have used and/or extended \libauto{}, a recommendation library that I worked on during the course of my PhD. The extended algorithms were added as part of my contribution to \libauto{} or LibRec itself. I will describe \libauto{}'s capabilities and architecture in details in Chapter \ref{librec-auto}.


\section{Context}

\todo{complete this, read the NSF case and the ai magazine talking about it}

Loan recommendation, as in the case of Kiva is a suitable candidate for the exploration of recommendation fairness due to several reasons. One of them is that fairness requirements are driven by the internal needs surrounding Kiva's philanthropic mission to provide equitable access to capital for its borrowers regardless of their demographic information such as gender, the country of origin, etc., as well as other characteristics e.g. the economic sector for which they need the loan. Another reason reason on the suitability of Kiva is as follows: Kiva's users will be more receptive of the fairness-oriented interventions in recommendations due to its philanthropic nature whereas the users of a movie streaming website such as Netflix, might not care to be fair to the movie producers. 

Movie recommendation might not be an obvious candidate for fairness-aware recommendation of course, since it is a domain of pure individual taste. Following the example of \cite{yao2017beyond}, our approach is to construct an artificial equity scenario within both MovieLens 1M and The Movies data for expository purposes only, with the understanding that real scenarios can be approached with a similar methodology. Note that this dataset is among the very few datasets with user demographic information for the use case of consumer-side fairness.

Furthermore, we will describe the choice of the protected attribute(s) and the transformation methods used to produce them in each dataset in this Chapter.


\section{Datasets}

The experiments were performed on the following publicly and/or proprietary datasets: MovieLens\cite{movielens}, The Movies dataset\footnote{https://www.kaggle.com/rounakbanik/the-movies-dataset}, and two variations of Kiva dataset: proprietary and publicly accessible data.
It's worth mentioning that an anonymized data was created based on the proprietary Kiva dataset which was used in my experimentation that we are allowed to share with the research community with Kiva's permission.
\todo{is it correct?}

 \todo{Yelp dataset. do we need that?}


The characteristics of the previously mentioned datasets are summarized in Table \todo{create a table for them}.  
% Dataset & #users, #items, #ratings, rating scale, density, consumer, provider, protected attribtue, access (public or private)

    \subsection{MovieLens 1M Dataset}
    For some of our experiments, we are using the well-known MovieLens 1M dataset~\cite{movielens}. This dataset is a movie rating dataset that was collected by the GroupLens research group. This dataset contains 1,000,209 ratings from 6,000 users on 4,000 movies. The sparsity of MovieLens is 4.47\%. All of the users in the MovieLens data set rated at least 20 movies.
    Besides user ratings, this dataset contains, some additional information on ratings, users and the movies. The rating information consist of ratings (only whole ratings), and the timestap of the ratings (in terms of seconds). The movie information consists of movie titles and 18 movie genre(s) (e.g. "Animation", "Comedy"). It also contains user information such as: gender (binary, represented by F and M), age (split into 7 buckets), 20 occupations (e.g. "academic/educator", "artist") and zip-code.
    
        \subsubsection{protected feature(s)}
        The choice of the protected features firstly depend on the stakeholder for which we are seeking fairness, and secondly it depends on the context and the previously detected unfairness or biases in the dataset. Since, there exists information about both the consumers (of the recommendations) and the providers (of the recommendations), it is possible to propose solutions both for the consumer-side and provider-side fairness.
        
        % balanced neighborhood
        In Chapter \todo{refer to balanced neighborhood}, we propose BLN-SLIM, an in-processing approach to mitigate both consumers' (BLN-SLIM-U) and providers' (BLN-SLIM-I) biases. Note that this approach takes a single binary protected attribute as input to the algorithm.
        
        Although, we use MovieLens dataset for only the consumer-side fairness for this experiment and the Kiva dataset for the provider-side fairness. We choose the movie genre as our protected attribute. One can use any other movie attribute. For example \cite{kamishima2018recommendation} chose the release year of the movie as the protected attribute.
        
        It can be seen in this data that there is a minority of female users (1709 out of the total of 6040). Certain genres display a discrepancy in recommendation delivery to male and female users. For example, in the ``Crime'' genre, female users rate a very similar number of movies (average of 0.048\% of female profiles vs 0.049\% of male profiles) and rate them similarly: an average rating of 3.7 for both female and male users. However, our baseline unmodified SLIM-U algorithm recommends in the top 10 an average of 1.10 ``Crime'' movies per female user as opposed to 1.18 such movies to male users. 
        
        Given that the rating profiles are similar but the recommendation outcomes are different, we can therefore conclude that the female users experience a deprivation of ``Crime'' movies compared to their male counter-parts. Similar losses can be observed for other genres. We are not asserting that there is any harm associated with this outcome. It is sufficient that these differences allow us to validate the properties of the BN-SLIM-U algorithm.
        
        In order to achieve genre discrepancy, within the MovieLens 1M dataset, we calculated the equity score (explained in \todo{where?}) and selected the five genres on which the baseline SLIM-U algorithm produced the lowest equity scores (e.i. $E_c@k$) : ``Film-Noir'', ``Mystery'', ``Horror'', ``Documentary'', and ``Crime''. Note that low equity scores are undesirable.
        
        \todo{should I summarize these explanations in the bln-slim chapter? if not, do I need to add a picture here?}



    \subsection{The Movies Dataset}
    
    This dataset is also a movie recommendation dataset which was obtained from the Kaggle website. It contains the metadata of 45,000 movies listed in the full MovieLens Dataset \footnote{https://grouplens.org/datasets/movielens} which were released on or before July 2017. As we mentioned before, movies are not a domain to which important fairness concerns are typically applied, we use this dataset as a well-known example with a rich set of provider-side features. The dataset contains 26 million ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5.
    
    From the movies' set of features, the following were used in this project: genres, original language, release date, revenue, run-time, popularity, production countries and spoken language. A sample of this dataset was extracted which contained the 559,070 ratings from 6,000 users on 14,623 items (density of 0.63\%).

    All the selected features were then transformed into categorical variables. If the movie's popularity is greater than the average popularity, we tag the movie as popular and unpopular otherwise. We transform the revenue and run-time in the same way as well. The release date is bucketed into old and new if the movie's release date is before or after 1990 \cite{kamishima2016model}. All the categorical features were transformed into dummy variables, resulting in a total of 323 binary features (21 genres, 2 popularity buckets (popular and unpopular), 2 year buckets (old, and new), 2 revenue buckets (low, and high), 2 run-time buckets (short, and long), 133 languages and 161 production countries).
    
        \subsubsection{Protected feature(s)}

        This dataset was used in Chapter \ref{fairness_postproc} \todo{reference or add info about the dynamic fairness thing} to incorporate a multi-concern fairness definition in a post-processing approach. In other words, the goal is to mitigate unfair exposure (either too high for some and too low for some) in multiple sensitive attributes (e.g. country, and economic sector, and gender, and etc.) for the providers at the same time. This dataset was also used in \todo{dynamic fairness}, to address all the fairness concerns one at a time.
    
        Since we need to address the unfairness issue in all of these features, we need to select the protected group within each one of them.
        We identified the following protected classes within each category: ``unpopular'' (popularity), ``lower revenue'' (revenue) , ``longer'' (running time), ``before 1990'' (release date), ``Horror'', ``Music'', ``Mystery'', ``History'' (genres), and ``CA'', ``ES'', ``DE'', ``HK'' (countries). These feature categories were chosen because they represented a minority within each category.
    
        This dataset is rich in movie metadata that one can use for the study of fairness in recommendation. As an example, this dataset contains the metadata about the cast and the crew for each movie: such as the gender (non-binary) information of the director(s) and the writer(s), the number of people from each gender in the director team or the writer team, etc. Although, we didn't use this information in our experiments.

    \subsection{Kiva dataset}
    \todo{changing our to my and we to I}
    This dataset is a proprietary dataset obtained from Kiva Microloans, a microlending site. And some of the collected data was extracted from the Kiva.org site using the site's API\footnote{http://build.kiva.org/}.
    % protected value: higher than average number of unfunded loans
    
    It includes all lending transactions over a 12-month period. Initially, there were 1,084,521 transactions involving 122,464 loans and 207,875 Kiva users.
    
    Of the 122,464 loans, we found that 116,650 were funded, that is they received their full funding amount from Kiva users by the 30-day deadline imposed by the site. Loans not fully funded within 30 days are dropped from the system and the money raised is returned to lenders.
    
    We selected only the funded loans for analysis. Each loan is specified by features including borrower's name/id, gender, borrower's country, loan purpose, funded date, posted date, loan amount, loan sector, and geographical coordinates such as geo-location, town, country and region of the borrower and the number of lenders who lent money to this loan.
    
    To reduce the feature space, and to solve the multicollinearity problem, highly correlated features were removed. The percentage funding rate (PFR) was added as a new feature, computed as follows:
    
    \begin{equation}
     \mbox{\textit{PFR}} =  \frac{1}{\mbox{\textit{{\#} days to fund}}} * 100 
    \end{equation}
    
    The percentage funding rate captures the speed at which a loan goes from being introduced in the system to being fully funded. For example, a loan with PFR of 25\% is accumulating a quarter of its needed capital each day. After preparing the data, the final features for each loan reduced to borrower's gender, borrower's country, loan purpose, loan amount (binned to 10 equal-sized buckets), and loan's percentage funding rate. 
    
    The transaction file included the lender's hashed id, lender's username, loan id, time of "purchase" or lending, the lent amount by that user, and the lending team id that that user is part of. A lender can be a part of one or several lending teams. These teams advocate for a specific cause for example agriculture, or are in support of a specific country such as Vietnam. We removed the lender user id, lending team id and the time of purchase id from the transactions file. The final columns were user's hashed id, loan id and the lent amount.
    
    
    We found that this dataset was highly sparse (density = $4.2e^{-5}$) and could not support effective collaborative recommendation, because a loan can only attract a limited amount of support (up to that needed for its funding). There are no ``blockbuster'' loans with thousands of lenders.
    
    To generate a denser dataset with greater potential for user profile overlap, we applied a content-based technique creating \textit{pseudo-items} that represent groups of items with shared features. We applied agglomerative hierarchical clustering \cite{rokach2005clustering} using the features of borrower gender, borrower country, loan purpose, loan amount (binned to 10 equal-sized buckets), and percentage funding rate (4 equal-sized buckets). We chose the cluster with the highest Silhouette Coefficient \cite{rousseeuw1987silhouettes} of around 0.69 which indicates a reasonable cohesion of the clusters. Note that there weren't any singleton clusters.
    
    After creating the feature file for psuedo loans, we transformed the lent amounts to ratings. We replace the loan id with the pseudo-loan id if that loan id exists in that cluster. Also, if a lender has lent to multiple loans in the same cluster(pseudo-loan), summation of all the lent amount is considered as one transaction since it shows how strong the interest of that lender to that item is.
    To transform the loan amount to ratings, for every user, we collected all the transactions on the pseudo-loans she has interacted with. We collected the lent amounts to each pseudo-loan and finally normalized it using the min-max normalization method in scikit-learn \cite{scikit-learn} to values between 3.0 and 5.0. Since ratings of 1s and 2s are considered as dislikes, since, if a lender hadn't liked a loan, she/he probably wouldn't have lent it any amount of money. Therefore, I set the minimum to 3.0. The min-max normalization is computed for every user separately over all her/his lent amounts.

    To make the dataset denser, we applied a 10-core transformation, selecting pseudo-items with at least 10 lenders who had funded at least 10 pseudo-items. The retained dataset has 2,673 pseudo-items, 4,005 lenders and 110,371 ratings / lending actions.
    
        % \subsubsection{kiva data collected from api}
        % The provider information in this Kiva dataset were collected from Kiva's API in September 2016. It contains approximately 1 million loans funded by approximately 180,000 lenders.

    \todo{true?}
    The final condensed dataset that were used in our experiments includes the pseudo items and the ratings behavior of lenders with them. Loan requests contain information about the borrowers, but the process of pseudo item creation hides and combines that information with the information of many other borrowers, therefore it protects the privacy of the borrowers. Also, no information about the lenders are relieved since their specific rating behavior is eliminated to a great extend. The final rating file essentially consists of the lenders' general lending behavior trends of users. In a way, both of our stakeholders (lenders and loans/borrowers) have now turned into pseudo-items and pseudo-users.
    As the privacy of the borrowers and the lenders are now protected, with the permission of Kiva, I have made this dataset available to use for the research community. This version of the Kiva dataset, is one of my contributions.\todo{yea?}


        \subsubsection{Protected features}
        In this dataset, we observed an imbalance within the following feature values/dimensions: (percentage funding rate), (country), (economic sector), (loan amount), (borrower gender). In keeping with Kiva's mission of providing equal access to capital across regions and economic sectors, we designate the items from the sectors and countries that have less than 1\% frequency in the training data as the protected group. As an example, 5 loan purposes in the economic sectors and 23 countries were selected to be the protected group. Note that we use Kiva dataset only for mitigating \textit{provider-side} unfairness either in one dimension or multiple-dimensions.
        
        We have used all these "fairness concerns" (or dimensions) in the OFAiR method explained in Chapter \ref{fairness_postproc} since OFAiR addresses multiple fairness concerns simultaneously. In Chapter \todo{where is dynamic fairness?}, we use all the previously mentioned fairness concerns, but unlike OFAiR, the proposed method, tries to mitigate bias in each fairness concern on at a time.
        
        In some of our research projects we have considered (region) instead of (country). Since these two features are highly correlated they both carry the same information and can be used interchangeably. In Chapter \ref{fairness_postproc} in the FAR/OFAR method and in Chapter \ref{fairness_inproc} in the Balanced neighborhood SLIM approach, we target the (region) feature.
        
        Kiva.org divides its borrowers into 9 geographic regions. We are defining the protected group as those regions of the world where it appears to be more difficult to fund loans. (In Kiva.org, a loan that does not attract enough lenders over a 30 day period is marked as unfunded and dropped from the system.) Therefore, we are promoting the loans that have not grabbed the lenders attention so far and help them get funded faster. Note that the assumption behind this method is that all the loan requests are equal regardless of all their information, in other words the more popular loans are not better than the less popular ones.
        
        In our dataset, we find that there are some geographic regions with a higher than average number of unfunded loans. In these regions, borrowers have a lower probability of getting the desired capital. As shown in Table~\ref{tab:unfunded}, the regions of North America, Eastern Europe, South America, and Asia have proportionately more funded loans than the regions of Africa, Middle East, and Central America\footnote{Our data set had only a single loan request from Australia.}. These regions where borrowers have lower funding percentages are treated as the protected group in our experiments.
        
        \begin{table}
            \centering
        \begin{tabular}{l|l|r}
            Category & Region & Unfunded \% \\ \hline
            Unprotected & North America & 1.73 \\
            & Eastern Europe & 0.99 \\
            & South America & 4.33 \\
            & Asia & 6.70 \\ \hline
            Protected & Africa & 10.57 \\
            & Middle East & 13.23 \\
            & Central America & 8.81 \\
        \end{tabular}
            \caption{Percentage of unfunded loans by region}
            \label{tab:unfunded}
        \end{table}
 

\begin{comment}
    \subsection{Yelp dataset}
    
    The original Yelp.com data included 6.7 million reviews from 1.6 million users on 192,609 businesses from different industries. We used a filtered subset, Yelp\_core40 \cite{mansoury2019bias}, which includes users who rated at least 40 businesses and businesses which were rated by at least 40 users. In the Yelp\_core40, there are many business categories  (e.g. car dealers, airports, or different types of restaurants, etc.). We limited the data set to restaurant/food establishments, using both "restaurant" and "food" labels for filtering. After pre-processing, the data set included 85,041 ratings by 1,355 users over 1,077 restaurants, covering 231 categories. The sparsity of Yelp is 5.83\%. The categories are defined by the labels provided in the original Yelp data set. Some of the categories are very popular (e.g. "American (New)", "Breakfast \& Brunch", etc.), while some of them are extremely specific (e.g. "Pretzels", "Shaved snow", etc.). There were also many establishments with multiple labels, which meant that they belonged to different item categories. 

\end{comment}

\section{Evaluation Metrics}
    \subsection{Accuracy}
    precision, recall, ndcg@10 or 5
        \subsubsection{Normalized Discounted Cumulative Gain}
        Therefore, we also evaluate the ranking accuracy of our algorithms in the results below. The measure that we use is normalized discounted cumulative gain (NDCG) measured at a specific list length. In this measure, an item appearing on a recommendation list accrues ``gain'' according to its position on the list -- thus the discount. The measure is normalized by comparing the algorithm's performance to the best ranking that could have been achieved. 

        Let $P_i@10$ be a list of retrieved list of length 10 and let $\tau$ be an indicator function that is 1 for movies that the user liked and 0 for others. Then, DCG@10 is computed as
        
        \begin{equation}
        DCG@10 = \sum_{k=1}^{10}{\frac{\tau(\rho_k)}{log_2(k+1)}}
        \end{equation}
        
        NDCG@10 is this DCG@10 value divided by the optimal DCG, which occurs when all of the movies liked by the user and appearing the test set are ranked at the top of the list in their order of preference.
        
        \textit{Normalized Discounted Cumulative Gain (nDCG).}
        Proposed in~\cite{jarvelin2002cumulated}, nDCG is a commonly-used measure of ranking quality, defined as Eq.~\eqref{eq:ndcg}.
        %We follow the convention in recommender systems that there is a gain in DCG (\emph{i.e.,} the item is \textit{relevant}) if the rating of the item in the list is positive.
        \begin{equation}
        \text{nDCG}=\frac{\text{DCG}}{\text{IDCG}},
        \label{eq:ndcg}
        \end{equation}
        where the formula of Discounted Cumulative Gain (DCG) of a $K$ ranked list is defined as
        \begin{equation}
        \text{DCG}=\sum_{i=1}^K\frac{{rel}_i}{\log(i+1)},
        \end{equation}
        where ${rel}_i$ is defined by the ratings of the item in the test set at position $i$. Ideal Discounted Cumulative Gain (IDCG) is the maximum possible DCG, which is the value of DCG computed by sorting all the items in the test set by their ratings.
        
    
    \subsection{Fairness}
        \subsubsection{Exposure-based Metrics}
        discounted proportional fairness?, etc.
        
        \subsubsection{Intra-list distance(ILD)}
        feature-based diversity both intra-list distance (ILD) 
        
        
        Our version of Intra List Distance \cite{eskandanian2016user} is an adjusted version of intra list similarity as introduced by Ziegler et al. \cite{ziegler2005improving}. Both capture the diversity of a list, and in our case, we used Euclidean distance as the underlying dissimilarity measure in ILD.
        
        \begin{equation}
            ILD(\mathcal{H}_u) = \frac{1}{|\mathcal{H}_u|(|\mathcal{H}_u|-1)}\sum_{i \in \mathcal{H}_u}\sum_{j \in \mathcal{H}_u}d(i,j)
        \end{equation}

        
        
        \subsection{Entropy}
        
        \subsection{Statistical Parity}
        The fairness of lists was evaluated based on protected group exposure, which measures the fraction of the recommendation list that consists of protected group items. This value is related to the fairness concept of ``statistical parity,'' measured relative to items' level of promotion within the recommender system. Because list lengths are fixed (10 in our case), the exposure of unprotected items is just one minus the protected group exposure.
        
        
        \subsubsection{accuracy-based metrics}
        calibration, equality of odds/chance, 
        
        
        \textbf{Consumer-side Risk Ratio}: In evaluating fairness of outcome, we use a variant of what is known in statistics as \textit{risk ratio} or \textit{relative risk} (RR)\cite{romei2014multidisciplinary}. We measure what is effectively \textit{relative opportunity}. In other words, we measure the observed probability of protected class items being recommended divided by the probability of unprotected class items being recommended.
        We construct a consumer-side equity score, $E_c@k$ for recommendation lists of k items, as the ratio between the outcomes for the different groups. Let $P_i@k = {\rho_1, \rho_2, ..., \rho_k}$ be the top $k$ recommendation list for user $i$, and let $\gamma()$ be a function $\rho \rightarrow \{0,1\}$ that maps to 1 if the recommended movie is in a protected genre. Then:

        \begin{equation}
        E_c@k=\frac{\sum_{i \in U^+}{\sum_{\rho \in P_i@k}{\gamma(\rho)}}/|U^+|}
        {\sum_{i \in U^-}{\sum_{\rho \in P_i@k}{\gamma(\rho)}}/|U^-|}
        \end{equation}
        
        $E_c@k$ will be less than 1 when the protected group is, on average, recommended fewer movies of the desired genre. It may be unrealistic to imagine that this value should approach 1: the metric does not correct for other factors that might influence this score -- for example, female users may rate a particular genre significantly lower and an equality of outcome should not be expected. While the absolute value of the metric may be difficult to interpret, it is still useful for comparing algorithms. The one with the higher $E_c@k$ is providing more movies in the given genre to the protected group. Note that this is an additive, utilitarian measure of outcome equity and does not take into account variations in user experience. More nuanced measures of distributional equity, including Pareto improvement, we leave for future work.
        
        
        \textbf{Provider-side Risk Ratio}: The provider-side equity score, $E_p@k$, is defined on recommendation lists of k items. Let $L^+$ be the set of loans in the test set that are from the protected regions, and $L^-$ be the corresponding set from the unprotected regions. Also, let $\pi^+()$ be an indicator function $\rho \rightarrow \{0,1\}$ that maps to 1 if the recommended loan is from a protected region and $\pi^-$ is a similar function for the unprotected regions. Then:

        \begin{equation}
        E_p@k=\frac{\sum_{i \in U}{\sum_{\rho \in P_i@k}{\pi^+(\rho)}}/|L^+|}
        {\sum_{i \in U}{\sum_{\rho \in P_i@k}{\pi^-(\rho)}}/|L^-|}
        \end{equation}
        
        $E_p@k$ will be less than 1 when loans from the protected regions are appearing less often on recommendation lists. As with $E_c$, this is a utilitarian measure, summing over all borrower regions, and does not speak to the the distribution across individual borrowers. Like $E_c$, it does not take the rank of recommended items into account.


        \textit{Average Coverage Rate (ACR) for Provider-side fairness}
        To measure the fairness for microlending, we compute the average number of borrower groups covered by the ranked list.
        \begin{equation}
            \text{ACR}=\frac{\sum_{u\in U_t}N_{S(u)}}{N_\text{bg}|U_t|},
        \end{equation}
        where $U_t$ is the test lender set, $|U_t|$ is the number of lenders in the test set, $N_\text{bg}$ is the total number of borrower groups and $N_{S(u)}$ is the number of borrower groups covered in the list $S(u)$. A larger ACR indicates a fairer system regarding borrower-side fairness.
        
        
        \textit{Discounted Proportional Fairness (DPF)}
        We adopt a well-accepted and axiomatically justified metric of fairness, the proportional fairness \cite{kelly1998rate}. Proportional fairness is a generalized Nash solution for multiple groups.
        
        \begin{equation*}
            DPF=\sum_{i=1}^{n_c}\log\left(\frac{x_c}{\sum_{c'}x_{c'}}\right),
        \end{equation*}
        
        where $x_c$ is the allocation utility of group $\mathcal V_c$. We define the utility of $\mathcal V_c$ by the cumulative gain that $\mathcal V_c$ received from all users,
        \begin{equation*}
        x_c=\sum_{u\in\mathcal U}\sum_{c}\sum_{i=1}^K\frac{{rel}_i\mathbbm{1}_{\{v\in \mathcal V_{c}\}}}{\log(i+1)}
        \end{equation*}

    \subsection{protected item exposure}
    rotected item exposure at different levels of nDCG loss: 1\%, 2\% and 3\%. We arrive at the exposure values in the table by assuming a locally-linear relationship of nDCG and fairness/exposure in between different $\lambda$ values, basically locating intercepts in the tradeoff graph.
    
    \subsection{Misalibration}\todo{do we need this part ?}
    As noted above, there is a close relationship between calibration and fairness, since miscalibration often disproportionately impacts users, as we show below. 
    Steck \cite{steck2018calibrated} has proposed a metric to measure recommendation miscalibration which is discussed in detail in \todo{what section??}. This metric measures whether recommender systems reflect the various interests of users relative to their initial preference proportions using the Kullback-Leibler (KL) divergence.
    The KL-divergence used in Steck's approach, however, measures the difference in preference distributions across item categories more generally than in \cite{tsintzou2018bias}.
    
    oted earlier, to evaluate the propagation effect we use the miscalibration metric proposed in \cite{steck2018calibrated}. It uses Kullback-Leibler (KL) Divergence to measure the difference between the preference distribution across all the item categories in a user profile and the distribution in the user's recommendation set. Both are based on the distribution of item categories $c$ for each item $i$, denoted by $p(c|i)$.
    
    \begin{itemize}
        \item $p(c|u)$: the distribution over categories $c$ of the set of items $\mathcal{H}_u$ interacted with by user $u$ in the past.
        \begin{equation}\label{input_preference}
            p(c|u) = \frac{\sum_{i \in \mathcal{H}_u} w_{u, i} \cdot p(c|i)}{\sum_{i \in \mathcal{H}_u} w_{u, i}},
        \end{equation}
    
        where $w_{i,u}$ is the weight of each item $i$, e.g. how recently it was liked or clicked on, or its popularity or rank.
        
        \item $q(c|u)$: the distribution across categories $c$ of the list of items recommended to user $u$.
        \begin{equation}
            q(c|u) = \frac{\sum_{i \in \mathcal{I}_u} w_{r(i)} \cdot p(c|i)}{\sum_{i \in \mathcal{I}_u} w_{r(i)}},
        \end{equation}
        
        where $\mathcal{I}_u$ is the set of recommended items and $w_{r(i)}$ is the weight of an item and can be measured by its rank $r(i)$ in the recommendation list.
    \end{itemize}
    
    KL-divergence \cite{kullback1997information} is used to measure the difference between these two probability distributions, or the divergence of $p$ from $q$. KL-divergence is denoted by:
    \begin{equation} \label{kl}
    MC_{KL}(p||q) = KL(p||\Tilde{q})= \sum_{c \in C}{p(c|u)\log\frac{p(c|u)}{\Tilde{q}(c|u)}},
    \end{equation}
    
    where $p(c|u)$ is the target distribution. If $q$ is similar to $p$, $MC_{KL}$ will take small values, and in the case of perfect calibration, it is 0. $MC_{KL}$ diverges if a category $c$ is $q(c|u)=0$ and $p(c|u)>0$, so instead we use:
    \begin{equation}
        \Tilde{q}(c|u) = (1 - \alpha) \cdot q(c|u) + \alpha \cdot p(c|u),
    \end{equation}
    
    where $0 < \alpha < 1$, so that $q \approx \Tilde{q}$. We set $\alpha = 0.01$ in this experiment.
    
    We rename this metric to $MC_{KL}$ instead of $C_{KL}$ which is described in \cite{steck2018calibrated}, since it specifies the degree to which we have miscalibration in our recommendations and it is more in line with the values that KL-divergence takes. For example, if $p$ and $q$ are very similar, KL-divergence takes lower values, so miscalibration is low and vice versa.
    
    One properties discussed in \cite{steck2018calibrated} is worth mentioning. $MC_{KL}$ is sensitive to small differences when $p$ is small. For example, if a user liked a category 2\% of the time and it is recommended to her 1\% of the time, $MC_{KL}$ considers it a significant change compared to a situation where a user likes a category 50\% of the time, while it's recommended to her 49\% of the time. 

    \todo{we need a summary of evaluation metrics in a table?}

\section{Experimental approach}
    n-fold, data cleaning, pre-processing 7 filtering, train/test etc.
    all the hyper parameters, librec-auto
    
    \subsection{Setup}
    % All the datasets were split into 80\% for training data and the remaining 20\% for the tested.
    % In the training phase of all the algorithms \todo{all?most}, 5-fold cross validation was used.
    All experiments were performed using a 5-fold cross validation setting where 80\% of each user's rating data is used for the training data set and the rest is used as the test data set (LibRec's \texttt{userfixed} configuration).
    
    We evaluated the algorithms based on the normalized Discounted Cumulative Gain (nDCG) of the top 10 recommended items, and for each algorithm we chose the hyperparameters that achieved the highest performance using the Grid Search method.
    
    For experiments, I used 80\% of each dataset as the training set and the other 20\% for the test. The training set was used for building a recommendation model and generating recommendation lists, and the test set was used for evaluating
    the performance of generated recommendations. As mentioned earlier, in this dissertation, two solutions for mitigating exposure bias in recommender systems are proposed: pre-processing and post-processing solutions. In pre-processing solution, the pre-processed training set is used as input
    for a recommendation algorithm, while in post-processing solution, longer recommendation lists generated by a recommendation algorithm is processed to generate the final shorter recommendation lists. In other words, I generated
    recommendation lists of size t = 50 (longer recommendation lists) for each user using each recommendation algorithm. I then extract the final recommendation lists of size n = 10 using the proposed and each reranking method by processing
    the recommendation lists of size 50.
    I used librec-auto and LibRec for running the experiments [67,123,125].
    
    
    
    
    \subsection{baseline recommendations}
        NMF(non-negative matrix factorization), UserKNN, itemKNN, BPR WRMF, Maxent, RankSGD, SLIM, FAR/PFAR, most-popualr, MMR, variants of ofair
        
        (a) RankSGD \cite{pmlr-v18-jahrer12b} uses stochastic gradient descent to optimize the ranking error; (b) UserKNN \cite{resnick1997recommender}  is a memory-based collaborative algorithm that computes user similarity; (c) Weighted Regularized Matrix Factorization (WRMF) ~\cite{hu2008collaborative} creates a reduced-dimensionality factorization of the rating matrix; (d) Maximum-entropy distribution (Maxent) \cite{choo2014gather} is a loan recommender system specially designed for Kiva. Maxent models lending behaviors by estimating a maximum-entropy distribution based on a set of heterogeneous information regarding micro-financial transactions available at Kiva.
        
        
        We also used MMR by itself, as a diversity-enhancing re-ranker, a variant of OFAiR that includes only user tolerance weights for each feature, and a variant that includes only the fairness weights for the protected feature dimensions without the tolerance weights. In this way, we can study separately the contribution of each of these aspects of the algorithm.
        
        
        The recommendation algorithms for generating the recommendation lists of size 10 in pre-processing solution are Biased Matrix Factorization (BiasedMF) [94], Singular Value Decomposition (SVD++) [93], and List-wise Matrix Factorization (ListRankMF) [160]. The recommendation algorithms for generating the longer recommendation lists of size 50 in post-processing solution are Bayesian Personalized Ranking (BPR) [146], Neural Collaborative Filtering (NCF) [71], User-based Collaborative Filtering (UserKNN) [147]. I chose these algorithms to cover different approaches in recommender systems: matrix factorization, neural networks, and neighborhood models.
        
        Each recommendation algorithm involves several hyperparameters. To identify the best-performing sets of hyperparameters for each algorithms, I performed gridsearch on hyperparameters space and selected the results with the highest precision for the next analysis. Table 5.2 shows the hyperparameter values that gridsearch was performed.
        
\section{Standardizing and automating Fairness related experiments}
????
