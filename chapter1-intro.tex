\chapter{Introduction}
\label{introchap}

% Robin's comments
    % more structure: subsections, giant block of text.
    % better argument for your case
    % the result of the arguments should be the research questions
    % some alternatives of the research questions

\todo[inline]{should I talk about information need of users and the fundamentals why recommender systems are a service and what they are? I don't do that here at all.}

\todo{research context}

After a period of substantially fast development in creating "smarter" digital systems, more and more decisions are being delegated to algorithms. Having observed the evolution, influence and consequences of these AI-supported systems over time, growing concerns has raised in the public discourse about ethical issues and the potential harms of these systems to individuals and the society. Machine learning is and continues to be one of the main pillars of these systems. 
 
Recommender systems are one of the most pervasive applications of machine learning in industry. They play a pivotal role in  connecting users to relevant items or content throughout the web while not only users rely heavily on them but also content producers, sellers or information providers. % in satisfying user's information needs
Although they are intended to assist people in different tasks, they can pose the risk of implicit or explicit discrimination against individuals or groups especially marginalized groups.

\section{Running example}
% \todo{change it to an example from Kiva}
Consider a loan recommender system that suggests loans to lenders to support. There are many online platforms attempting to connect small businesses or entrepreneurs who have requested for a loan to lenders or donors in some way. 

Discriminatory recommendations in this system could mean that for instance, more attractive, lighter-skinned, and less obese borrowers are favored \cite{JENQ2015234}, and their loans get funded faster. Or when they get similar recommendations, it is just because of their similar demographic information not because of reasons they have provided about why they need the fund.

The system would therefore need to defend against biases in recommendation output, even biases that arise due to behavioral differences: for example, certain regions in the world are more favored due to the historical or political relationships of the countries where the lenders and borrowers reside. Therefore, certain borrowers unjustly get a high exposure to the lenders and the others might not get any visibility at all. And due to the formed positive feedback loops, this gap might get wider over time, resulting in ignoring certain user subgroups.


% Consider a recommender system suggesting job opportunities to job seekers. Many online platforms attempt to connect job employments and job applicants in some way. Sometimes they are professional networks which have a job-seeking component such as Xing and LinkedIn, or they might have been designed only for employment seeking.

% Discriminatory recommendations in this system could mean that men and women with similar qualifications don't get recommendations of jobs with similar rank and salary. Or when they get similar recommendations, it is just because of their demographic information not because of their qualifications. For examples, if there are more women on a job recommendation website and they all apply for secretarial jobs, these jobs might show up in the recommendations of women who are looking for CEO jobs. On the other side of the coin, jobs with better salaries and higher ranks will be recommended to men regardless of their qualifications. The system would therefore need to defend against biases in recommendation output, even biases that arise due to behavioral differences: for example, male users might be more likely to click optimistically on high-paying jobs. \todo{more clarifications?}

\textbf{Unfairness towards multiple stakeholders}

Since recommender systems are multi-stakeholder settings, discrimination in these systems might be against any of the stakeholders involved in the system transactions such as users who we also call the consumers of the recommendations or content, providers or content producers, the system itself and any other party. In the loan recommendation example, discrimination could be against the lenders by not presenting a diverse set of recommendation to them.

\todo[inline]{should i talk about the presentation bias instead of filter bubble?}

\section{Unfairness in recommender systems}

% potntial underlying reasons for recsys to be unfair
There are multiple underlying reasons that cause the recommendation algorithms to be discriminatory. For example, if recommender systems are trained on biased data, the outcomes might get biased a.k.a. "garbage in, garbage out". 

These algorithms might also propagate the existing biases in the data \cite{barocas2016big}. As an example, over time, recommender systems might homogenize the recommendation lists of users. In other words the users will be trapped in their filter bubbles a.k.a. echo chambers. This phenomenon is discriminatory to the users of the recommender systems as their choices becomes very limited and too homogeneous overtime, preventing them to see novel content. This issue is not only discriminatory towards users, but it is also discriminatory towards content providers as only a small group of them gain popularity while others are ignored.

%real world example
In a music streaming platform, this issue could affect the livelihood of musicians. Another real-world example of the harms that filter bubbles bring about, is the influence of the news feeds on peoples minds that can eventually lead to political polarization \cite{HONG2016777}.  

% Over-personalization in Youtube's recommendation algorithms has also caused harm to suicidal teenagers by showing them similar videos on different ways of committing suicide. 

Ignoring the societal and ethical consequences of recommendation algorithms in different contexts leads to algorithm designs that can be unfair against their users.



% This type of bias occurs because users are more likely to interact with items that the system presents to them. 
    
% The first issue here is the item selection by the recommender system here and whether it will contribute to unfairness to any of the stakeholders. We have addressed how to mitigate this issue in Chapter \ref{fairness} in the pre-processing section \todo[inline]{preprocessing section}. 
    
% The second issue is that presentation bias, can lead to a form of positive feedback loop, in which presented items gain more popularity since they are more likely to be interacted with. This leads to greater bias towards presenting the items when the popular items are promoted more at the cost of other items. Presentation bias and the created feedback loop not only magnifies the initial differences between items' popularity but also it makes it hard for new providers to attract the attention of users to their products/items in a system with this type of bias.
    
% For example, in the microlending case, if the system doesn't recommend loans from a specific geographical region because on average the requested loans from this region are risky (their borrowers are less likely to return the loan), not only the current good borrowers (the borrowers who are more likely to return the loan) from that region are affected, but also the future good borrowers. This positive feedback loop re-enforces over time until that region is completely ignored by the system.
    

\section{Fairness-aware recommender systems}

In the recent years, the topic of unfairness in machine learning algorithms, has gained the attention of a multidisciplinary community from computer scientists, social scientists to legal scholars. Thus as a response, recent research has shifted from design of algorithms that pursue purely optimal outcomes with respect to an objective function (e.g. accurate outcomes) into ones that also consider algorithms' social impacts such as unfairness, discrimination, etc.

The problem of mitigating unfairness in recommender systems has its unique challenges that set it apart from other machine learning problems. Therefore, the methods that have been developed in other machine learning fairness literature are not fully applicable in recommender systems.

The multi-stakeholder nature of these systems, and the centrality of personalization and its conflict with accuracy-based goals are among the core issues that we try to address here while proposing solutions.

\section{Accuracy / fairness tradeoff}

Traditionally, the main goal of a recommender system is personalization. In order to achieve satisfactory personalization, these systems optimize for the accuracy.
% what accuracy is? and what word should we replace it with?
% In machine learning algorithms, achieving optimum accuracy by itself is impossible and might not even be desirable! 

In fact, what recommender systems are seeking for, is the ability to predict the items that a user will like in the future, or in other words, the ability to present the items that are more likely to be interacted with by the user. Although, achieving optimum accuracy by itself is impossible or in some cases it might not be the only desirable goal.
\todo[inline]{why?}

\subsection{Impossibility of ideal accuracy}
So, recommender systems recognize the more general patterns in the data and use this information for recommendation generation. This general pattern is based on the log history of user rating behavior in the system. So, these general patterns might not satisfy the short term interests of users or the evolution of their taste over time. These changes are in fact hard to predict as there are many underlying confounding factors re-shaping them.

% In practice, it is hard to predict the information need of users at each point in time. 
In fact, users themselves might not even know what they want. For example, It would be odd to think that every time a user opens a music streaming platform, she will exactly know the songs she wants to play. So, achieving ideal accuracy is unlikely in spite of optimizing the objective function to increase it.

% For example, when you want to listen to music, a lot of the times you don't think about the songs you exactly want to listen to. You might not want to listen to what you were listening before. Therefore, we can never achieve perfect accuracy and presenting accurate recommendations to the users is a myth.

Increasing accuracy more than a threshold might even be undesirable or unsatisfactory to one or all the stakeholders of a recommender system. This is because, increasing accuracy can contribute to propagating and re-enforcing some of the existing biases such as popularity bias, positive feedback loops, filter bubbles, miscalibration error, over- and under- estimation of accuracy, and etc. 
% Below are some examples of the potential biases that increasing accuracy causes for consumers and providers.
\todo[inline]{check all the biases mentioned before..}


\subsection{Accuracy \& data imbalance}
Accuracy-based metrics can hide consumer unfairness in their outcomes. Accuracy mostly reflects the quality of the recommendations that the majority of the user base experiences regardless of what other minority user groups are experiencing. Therefore, minority groups might not receive a high quality service from the system. 

That is because these metrics are usually averaged over all the user base. And usually, the previously collected data is not reflective of the diversity in the society due to various reasons such as bias in data collection or simply lack of online participation from the minority groups. Therefore, the data imbalance can create accuracy imbalance between the majority user group(s) on whom we have more data and minority groups with less data. So, having a discrepancy in recommendation quality for different user groups is a type of unfairness  in recommender systems and reaching to balance it, is of interest for the research community \todo{cite the people who have done this}.


\subsection{Accuracy \& popularity bias}
Recommendation datasets are usually inter-weaved with popularity bias. Striving to reach a high accuracy, will lead to reinforcing this bias as well. Over time, this bias gets even worse due to effect of positive feedback loops in the system.

As a result, popular items will gain more popularity and will be recommended more often. 
Therefore users will receive more and more homogeneous recommendations over time. The created filter bubble and presenting users with limited options is unsatisfactory to users. The previous research in the literature shows that increasing diversity in the recommendations increases user satisfaction, while we know diversity has a trade-off with accuracy. Aside from that, filter bubbles implicitly force users to consume the same content which can have harmful social impacts (polarizing the society based on political views, or recommending more violent videos to kids after watching only one video with the same content). 

Not only the consumers of recommendations will be affected, but the item providers might also suffer. Popularity bias pushes the less popular item-providers away and leaves them with less consumer exposure opportunities. This unjust exposure can affect their livelihoods.


\subsection{Balancing accuracy and fairness}

In spite of all the previously mentioned issues about accuracy, reaching a high accuracy is still at the core of personalization. Because accuracy is known to be strongly tied with user satisfaction (satisfying users' information need) of the system and also because compared to other goals like fairness or diversity, accuracy is financially more beneficial therefore necessary for the survival of the commercial recommendation systems (e-commerce websites).

Although, since the end goal for the recommender system is to serve its users (any stakeholder group), any potential harm in the system should be prevented and the biases towards its users should be mitigated. Integrating and operationalizing a goal such as fairness to the system is essential but challenging especially because it has major tradeoffs with accuracy.


\todo[inline]{add this => However, optimizing recommendation accuracy often comes at the expense of provider fairness, due to various biases present in recommender systems, including popularity bias \cite{celma2008hits,lee2014fairness}, and user-base composition \cite{lin2019crank, yao2017beyond}. Research in provider fairness is therefore generally concerned with improving the tradeoff between fairness and accuracy, or in other words, increasing the amount of fairness that can be gained for a given degree of accuracy loss.
}
% \begin{comment}
% % bit and pieces
% Although increasing accuracy for better personalization can be a double edged sword. 
% On the one hand, accuracy is known to be strongly tied with user satisfaction and is financially conducive for the system. On the other hand, increase in accuracy might contribute to

% The end goal for the system is to serve users and benefits them. While in many cases, due to biases in the data (such as popularity bias) or the bias propagation over time, etc. more than a certain level of accuracy will become harmful to users from other aspects. Some of these other aspects are measures by beyond-accuracy measures such as diversity, fairness, serendipity.
% The previous research in the literature shows that increasing diversity in the recommendations increases user satisfaction, while we know diversity has a trade-off with accuracy.

% These issue complicate the research problems more since, we need to understand these aspects of the problem in order to meticulously identify the context of the problem, the type of unfairness we can address, and the the family of solutions that we want to adopt in order to operationalize fairness in recommender systems.
% \end{comment}

% kiva? non-profit context? 

% how is this problem and set of solutions different than the ones that are proposed to solve popularity bias?


Therefore, addressing the accuracy and fairness trade-off as competing goals is essential for all the stakeholders of a system. Throughout this dissertation, I propose empirical solutions that give more control over this trade-off and strives to find a sweet spot between them. 


\subsection{Fairness interventions}

Fairness is a convoluted concept and it can have many different definitions under different contexts. More details on these concepts will be provided in Chapter \ref{fairness}. Our definitions of fairness are group-based with a focus on fairly increasing the exposure of under-privileged providers or consumers. 

With fairness concerns identified, it becomes possible to consider interventions to improve recommender systems performance relative to them. 

In doing so, it is worth keeping in mind the ``traps'' identified in \cite{selbst2019fairness}, possible hazards in applying fairness concepts in sociotechnical systems. Sebst and colleagues note that sometimes a technical fix is not always the most appropriate approach for problems of power imbalance and bias, and the failure to recognize this is defined as the \textit{solutionism trap}. There may be a wide variety of non-computational solutions to problems that surface themselves as unfair recommendations. 

Generally, three types of solutions have been proposed to operationalize fairness notions in order to mitigate  unfairness: pre-processing, in-processing, and post-processing.

Pre-processing methods focus on compensating for the existing biases in a dataset. They suggest different data collection enhancements to compensate for the biases that occur due to data imbalance. In-processing approaches try to improve the fairness of results by integrating fairness notions into recommendation generation itself. Post-processing approaches focus on modifying the outputs of algorithms to satisfy a fairness criterion. More details on these methods can be found in Chapter \ref{fairness}.

We propose one pre-processing, one in-processing, and three post-processing approaches to improve the fairness-accuracy trade-off. More details are presented in Chapter \ref{fairness_inproc}, \ref{fairness_postproc}, and in Chapter \ref{conclude}.

% Throughout this dissertation, we propose different approaches to address achieving a better trade-off between accuracy and fairness. 

We run our experiments and analyze our results in the e-commerce and philanthropic contexts which are explained in Chapter \ref{chap:methodology}.


% For example, Kiva is a non-profit organization that operates a crowd-sourced microlending platform with the goal of financial global inclusion. In Kiva, the sides of the interaction are the borrowers, generally developing-world entrepreneurs who seek small amounts of capital to enhance their business capacity, and lenders, who are the application's end-users. A typical lender will contribute only a fraction of the total amount for any one loan, but may support multiple loans at any one time. Lenders do not get any interest on their investments and so supporting a Kiva borrower is essentially a philanthropic act. Kiva's mission emphasizes equitable access to capital for its borrowers, who generally cannot make use of traditional forms of banking and lending~\cite{choo2014gather}. Lenders are the users of the recommender system, which has the purpose of lowering their search costs in finding borrowers whose goals and needs appeal to them. 

% explaining the fairness issues
% Several provider-side fairness concerns might arise in this recommendation context.\footnote{Consumer-side fairness has not arisen so far as a concern in this application.} One key concern, arising from Kiva's mission of supporting world-wide access to capital, is that the geographic imbalances in users' preferences may manifest themselves in the disproportionate representation of certain countries or regions in recommendation lists. This could give rise to a positive feedback loop, as the recommended items are more likely to be supported, and thus the lending becomes even more highly concentrated. A similar kind of imbalance may arise with respect to different industries or economic sectors. Thus, we can identify at least two fairness concerns within this recommender system: equity in geographic distribution of capital \cite{liu2019personalized}, and equity across economic sectors \cite{sonboli2020opportunistic}.


\section{Research questions}

%incomplete.. question and brief solution and the problems that it follows
The research questions that we seek to answer are as follows:

\begin{itemize}
    \item How can we characterize the fairness/accuracy trade-off in different algorithms?
    \todo[inline]{how does different algorithms compare (all rerankers), which algorithm should I use?}
    \item How can we incorporate fairness objectives into recommendation generation or post-processing to achieve better trade-offs? % far/pfar, ofair
    \item How can we extend these ideas to more complex scenarios involving multiple fairness concerns or requiring dynamic assessment of fairness opportunities? %dynamic fairness, multiple fairness concerns
    \item How can we ensure the reproducibility of the results and widen availability of fairness-aware techniques in recommendation? %librec
\end{itemize}

\section{Summary of contributions}
\todo[inline]{which chapter answers these research questions.}


% 1) Can we maintain a high level of accuracy for the user base while increasing provider fairness? Or consumer fairness?

% 2) How can we control the accuracy trade-off better for providers? How to preserve accuracy while doing that? 

% 3) How can we control the accuracy fairness trade-off better for providers when addressing multiple fairness concerns? And can we use user propensities towards different categories to preserve accuracy as much as possible?

% 4) how can we achieve this balance dynamically?

% 5) What if we have data constraints? data minimization
% 6) Who controls the trade-off between accuracy and fairness? Should users have some agency over it? Should it be transparent to the users? Will re-rankers provide more transparency?


%%%%% incomplete


% This trade-off can have different underlying reasons. firstly, accuracy/personalization is a consumer-centric goal most of the time, and for example provider-side fairness is provider side. due to popularity bias, increasing personalization, might lead to less diversity for providers. and eventually less diversity for the consumers themselves. Achieving different goals at the same time might not be even possible. one other reason is the multi-party settings that recommendations offer, every one is striving to achieve what's best for them, and these goals might be competing. providers want one thing, the consumers want another thing. Therefore it's important to design methods to find a balance between these goals.



\todo{Do we need a summary of Contributions?}
\todo[inline]{DO we need future work section?}

