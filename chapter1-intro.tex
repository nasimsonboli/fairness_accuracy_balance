\chapter{Introduction}
\label{introchap}


\section{one}

\begin{comment}
example 
problems that exist
approaches
\end{comment}


After a period of substantially fast development in creating "smarter" digital systems, more and more decisions are being delegated to algorithms. Having observed the evolution, influence and consequences of these AI-supported systems over time, growing concerns about ethical issues and the potential harms of these systems to individuals and the society has raised in the public discourse.

Machine learning is and continues to be one of the main pillars of the "smart" web. Recommender systems are one of the most pervasive applications of machine learning in industry. Although these systems are intended to assist people in different tasks, they can pose the risk of implicit or explicit discrimination against individuals or groups especially minorities.


% example
Recommender systems play a pivotal role in connecting users to relevant items or content throughout the web while not only users rely heavily on them but also content producers, sellers or information providers. 

\todo{change it to an example from Kiva}
Consider a recommender system suggesting job opportunities to job seekers. Many online platforms attempt to connect job employments and job applicants in some way. Sometimes they are professional networks which have a job-seeking component such as Xing and LinkedIn, or they might have been designed only for employment seeking.

Discriminatory recommendations in this system could mean that men and women with similar qualifications don't get recommendations of jobs with similar rank and salary. Or when they get similar recommendations, it is just because of their demographic information not because of their qualifications. For examples, if there are more women on a job recommendation website and they all apply for secretarial jobs, these jobs might show up in the recommendations of women who are looking for CEO jobs. On the other side of the coin, jobs with better salaries and higher ranks will be recommended to men regardless of their qualifications. The system would therefore need to defend against biases in recommendation output, even biases that arise due to behavioral differences: for example, male users might be more likely to click optimistically on high-paying jobs. \todo{more clarifications?}



Since recommender systems are multi-stakeholder settings, discrimination in these systems might not only be against the users, but other stakeholders e.g. content producers. 

In the job recommendation example that was mentioned before, besides the users who receive the recommendations, the companies or institutes that offer the jobs are the other involved party. These systems can be discriminatory against the job creators, not giving them enough exposure to the job applicants. Small businesses might struggle to attract applicants in these websites as they are recommended less to the qualified users.


There are multiple underlying reasons that cause the recommendation algorithms to be discriminatory. For example, if recommender systems are trained on biased data, the outcomes might get biased a.k.a. "garbage in, garbage out". These algorithms might also propagate the existing biases in the data \cite{barocas2016big}. As an example, recommender systems can over-personalize the recommendation lists of users or in other words trap users in their filter bubbles or echo chambers. This phenomenon is discriminatory to the users of the recommender systems as their choices becomes more and more limited and too similar to each other. Not only it is discriminatory towards users, but it might also be discriminatory towards the providers of items as only a few of them are shown to the users. If this happens in a music recommendation system, it can affect the livelihood of musicians. Another real-world example of the harms of filter bubbles is the influence of the news feeds on peoples minds that can eventually lead to political polarization \todo{cite Facebooks ads}. Over-personalization in Youtube's recommendation algorithms has also caused harm to suicidal teenagers by showing them similar videos on different ways of committing suicide. 

Ignoring the societal and ethical consequences of these algorithms in different contexts leads to algorithm designs that can be unfair against their users.

In the recent years, these issues in machine learning algorithms in general, has gained the attention of a multidisciplinary community from computer scientists, social scientists to legal scholars. Thus as a response, recent research has shifted from design of algorithms that pursue purely optimal outcomes with respect to an objective function (e.g. accurate outcomes) into ones that also consider algorithms' social impacts such as unfairness, discrimination, etc.

The problem of mitigating unfairness in recommender systems has its unique challenges that set it apart from other machine learning problems. Therefore, the methods that have been developed in other machine learning fairness literature is not fully applicable in recommender systems.

But, in order to thinking about approaching to propose solutions to the unfairness issues in recommender systems we need to acknowledge the challenges we face in recommender systems such as the multi-stakeholder nature of these algorithms, the centrality of personalization and its conflict with accuracy-based goals, the role of user response \todo{???} and the ranked outputs \todo{???}.

%provider-side unfairness
Traditionally, the main goal of a recommender system is personalization. In order to achieve satisfactory personalization, these systems optimize for the accuracy.

% WHAT TO REPLACE THE TERM ACCURACY WITH?
% what is accuracy and what we mean by it
% sth that you click on or highly relevant to you
% providing better lists
% diminishing returns....

% what accuracy is? and what word should we replace it with?
Achieving optimum accuracy by itself is impossible and might not even be desirable! What we are seeking for is to be able to predict what user will like in the future, or in other words present items to the users that are more likely to be clicked on. We might be able to recognize the general patterns in the data using the recommendation models, but in reality it is hard to predict what users actually want at each point in time. As a matter of fact, users themselves might not know what they want at each time. For example, when you want to listen to music, a lot of the times you don't think about the songs you exactly want to listen to. You might not want to listen to what you were listening before. Therefore, we can never achieve perfect accuracy and presenting accurate recommendations to the users is a myth.

Even increasing the accuracy more than a threshold might be undesirable or even unsatisfactory to the users. Since, higher accuracy (more than a threshold) can contribute to propagating the existing biases such as popularity bias, filter bubbles, etc. As we achieve for more accuracy we might reinforce the filter bubbles over time \todo{cite}. In this case, consumers are unsatisfied because of too much homogeneity in their recommendations \todo{filter bubbles literature} and are more satisfied with the increased diversity in their recommendation lists \todo{cite the paper that says diversity is desired by the users}. Not only the users or the consumers of recommendations, but also other stakeholders such as item providers might suffer. Less diversity in users' recommendation lists could mean less item-providers get exposure to the users. This unjust exposure can affect their livelihoods.

% consumer side unfairness
Additionally, accuracy-based metrics themselves can hide consumer unfairness in their outcomes. The calculated average accuracy mostly reflects the quality of the recommendations that the majority of the user base experience regardless of what other minority user groups are experiencing. 
That is because these metrics are usually computed over all the user base. And usually, the gathered data is not reflective of the diversity in the society due to various reasons. Therefore, the data imbalance can create accuracy imbalance between the majority user group(s) on whom we have more data and other minority groups with less data. So, having a discrepancy in recommendation quality for different user groups is considered as unfairness and reaching to balance it, is of interest for the research community \todo{cite the people who have done this}.

Since the end goal for the recommender system is to serve its users (that might belong to any stakeholder group), any potential harm in the system should be prevented and the biases towards its users should be mitigated. Integrating and operationalizing a goal such as fairness to the system is essential but challenging especially because it has tradeoff with accuracy.

% The previous research in the literature shows that increasing diversity in the recommendations increases user satisfaction, while we know diversity has a trade-off with accuracy.

In spite of all the previously mentioned issues about accuracy, reaching a high accuracy is still at the core of personalization not only because it is known to be strongly tied with user satisfaction but also because compared to other goals like fairness, accuracy is financially more beneficial and necessary for the survival of the commercial recommendation systems (e-commerce websites).


\begin{comment}
% bit and pieces
Although increasing accuracy for better personalization can be a double edged sword. 
On the one hand, accuracy is known to be strongly tied with user satisfaction and is financially conducive for the system. On the other hand, increase in accuracy might contribute to

The end goal for the system is to serve users and benefits them. While in many cases, due to biases in the data (such as popularity bias) or the bias propagation over time, etc. more than a certain level of accuracy will become harmful to users from other aspects. Some of these other aspects are measures by beyond-accuracy measures such as diversity, fairness, serendipity.
The previous research in the literature shows that increasing diversity in the recommendations increases user satisfaction, while we know diversity has a trade-off with accuracy.

These issue complicate the research problems more since, we need to understand these aspects of the problem in order to meticulously identify the context of the problem, the type of unfairness we can address, and the the family of solutions that we want to adopt in order to operationalize fairness in recommender systems.
\end{comment}


% kiva? non-profit context? 

% how is this problem and set of solutions different than the ones that are proposed to solve popularity bias?

%not theoretical

Therefore, addressing the accuracy and fairness trade-off as competing goals is essential for all the stakeholders of a system. Throughout this dissertation, I propose empirical solutions that give more control over this trade-off and strives to find a sweet spot between them. 

Fairness is a convoluted concept and it can have many different definitions under different contexts. More details on these concepts will be provided in Chapter \ref{fairness}. Our definitions of fairness are group-based with a focus on fairly increasing the exposure of under-privileged providers or consumers. 
% Most of the proposed solutions focus on provider-side fairness with one work addressing the consumer-side fairness.

\todo{revise}
With fairness concerns identified, it becomes possible to consider interventions to improve recommender systems performance relative to them. In doing so, it is worth keeping in mind the ``traps'' identified in \cite{selbst2019fairness}, possible hazards in applying fairness concepts in sociotechnical systems. Sebst and colleagues note that sometimes a technical fix is not always the most appropriate approach for problems of power imbalance and bias, and the failure to recognize this is defined as the \textit{solutionism trap}. There may be a wide variety of non-computational solutions to problems that surface themselves as unfair recommendations. 


% briefly talking about approaches
Generally, three types of solutions have been proposed to operationalize fairness notions in order to mitigate  unfairness: pre-processing, in-processing, and post-processing.

Pre-processing methods focus on compensating for the existing biases in a dataset. They suggest different data collection enhancements to compensate for the biases that occur due to data imbalance. In-processing approaches try to improve the fairness of results by integrating fairness notions into recommendation generation itself. Post-processing approaches focus on modifying the outputs of algorithms to satisfy a fairness criterion. More details on these methods can be found in Chapter \ref{fairness}.

We propose one in-processing, three post-processing and one pre-processing method to improve the fairness-accuracy trade-off. More details are presented in Chapter \ref{fairness_inproc}, \ref{fairness_postproc}, and in Chapter \ref{conclude}.

% Throughout this dissertation, we propose different approaches to address achieving a better trade-off between accuracy and fairness. 

We run our experiments in both the e-commerce and philanthropic contexts.
\subsection{Example Applications}

\todo{revise and make it coherent with the rest of the material}

As e-commerce has become a dominant shopping avenue, so have online sites for philanthropic activity. 

For example, Kiva is a non-profit organization that operates a crowd-sourced microlending platform with the goal of financial global inclusion. In Kiva, the sides of the interaction are the borrowers, generally developing-world entrepreneurs who seek small amounts of capital to enhance their business capacity, and lenders, who are the application's end-users. A typical lender will contribute only a fraction of the total amount for any one loan, but may support multiple loans at any one time. Lenders do not get any interest on their investments and so supporting a Kiva borrower is essentially a philanthropic act. Kiva's mission emphasizes equitable access to capital for its borrowers, who generally cannot make use of traditional forms of banking and lending~\cite{Choo_understanding_kiva}. Lenders are the users of the recommender system, which has the purpose of lowering their search costs in finding borrowers whose goals and needs appeal to them. 

% explaining the fairness issues
Several provider-side fairness concerns might arise in this recommendation context.\footnote{Consumer-side fairness has not arisen so far as a concern in this application.} One key concern, arising from Kiva's mission of supporting world-wide access to capital, is that the geographic imbalances in users' preferences may manifest themselves in the disproportionate representation of certain countries or regions in recommendation lists. This could give rise to a positive feedback loop, as the recommended items are more likely to be supported, and thus the lending becomes even more highly concentrated. A similar kind of imbalance may arise with respect to different industries or economic sectors. Thus, we can identify at least two fairness concerns within this recommender system: equity in geographic distribution of capital \cite{liu2019personalized}, and equity across economic sectors \cite{Sonboli202Oofair}.


%incomplete.. question and brief solution and the problems that it follows
The research questions that we seek to answer are as follows:

1) Can we maintain a high level of accuracy for the user base while increasing provider fairness? Or consumer fairness?

2) How can we control the accuracy trade-off better for providers? How to preserve accuracy while doing that? 

3) How can we control the accuracy fairness trade-off better for providers when addressing multiple fairness concerns? And can we use user propensities towards different categories to preserve accuracy as much as possible?

4) how can we achieve this balance dynamically?

5) What if we have data constraints? - data minimization
6) Who controls the trade-off between accuracy and fairness? Should users have some agency over it? Should it be transparent to the users? Will re-rankers provide more transparency?


%%%%% incomplete


\subsection{Summary of Contributions}

