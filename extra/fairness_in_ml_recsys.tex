% \subsection{Fairness}
% \subsection{fairness in ML}
\subsection{fairness in ML and Recommendation}

\subsubsection{Personalization}

The dominant recommendation paradigm, collaborative filtering, uses user behavior as its input, ignoring user demographics and item attributes~\cite{koren2015advances}. However, this does not mean that fairness with respect to such attributes is irrelevant. Consider a recommender system suggesting job opportunities to job seekers. The operator of such a system might wish, for example, to ensure that male and female users with similar qualifications get recommendations of jobs with similar rank and salary. The system would therefore need to defend against biases in recommendation output, even biases that arise due to behavioral differences: for example, male users might be more likely to click optimistically on high-paying jobs.

Defeating such biases is difficult if we cannot assert a shared global preference ranking over items. Personal preference is the essence of recommendation especially in areas like music, books, and movies where individual taste is paramount. Even in the employment domain, some users might prefer a somewhat lower-paying job if it had other advantages: such as a shorter commute time, or better benefits. Thus, to achieve the policy goal of fair recommendation of jobs by salary, a site operator will have to go beyond a personalization-oriented approach, identify key outcome variables such as salary, and control the recommendation algorithm to make it sensitive to these outcomes for protected groups.

\subsubsection{Multiple stakeholders}

As the example of job recommendation makes clear, a recommender system is often in the position of facilitating a transaction between parties, such as job seeker and prospective employer. Fairness towards both parties may be important. For example, at the same time that a job recommender system is ensuring that male and female users to get recommendations with similar salary distributions, it might also need to ensure that jobs at minority-owned businesses are being recommended to the most desirable job candidates at the same rate as jobs at white-owned businesses.

A \textit{multistakeholder recommender system} is one in which the end user is not the only party whose interests are considered in generating recommendations~\cite{soappaper,abdollahpouri_recommender_2017}. This term acknowledges that recommender systems often serve multiple goals and therefore a purely user-centered approach is insufficient. Bilateral considerations, such as those in employment recommendation, were first studied in the category of \textit{reciprocal recommendation} where a recommendation must be acceptable to both parties in a transaction~\cite{akoglu_valuepick:_2010}. Other reciprocal recommendation domains include on-line dating~\cite{reciprocal}, peer-to-peer ``sharing economy'' recommendation (such as AirBnB, Uber and others), on-line advertising \cite{targetadvertisingbiding}, and scientific collaboration~\cite{lopes2010collaboration,tang2012cross}.

When recommendations must account for the needs of more than just the two transacting parties, we move beyond reciprocal recommendation to multistakeholder recommendation. Today's web economy hosts a profusion of multisided platforms, systems of commerce and exchange that bring together multiple parties in a marketplace, where the transacting individuals and the market itself all share in the transaction~\cite{evans_matchmakers:_2016}. These platforms must by design try to satisfy multiple stakeholders. Examples include LinkedIn, which brings together professionals, employers and recruiters; Etsy, which brings together shoppers and small-scale artisans; and Kiva.org, which brings together charitably-minded individuals with third-world entrepreneurs in need of capital.

\subsubsection{Stakeholder utility}

Different recommendation scenarios can be distinguished by differing configurations of interests among the stakeholders. We divide the stakeholders of a given recommender system into three categories: consumers $C$, providers $P$, and platform or system $S$. The consumers are those who receive the recommendations. They are the individuals whose choice or search problems bring them to the platform, and who expect recommendations to satisfy those needs. The providers are those entities that supply or otherwise stand behind the recommended objects, and gain from the consumer's choice.\footnote{In some recommendation scenarios, like on-line dating, the consumers and providers are same individuals.} The final category is the platform itself, which has created the recommender system in order to match consumers with providers and has some means of gaining benefit from successfully doing so. 

Recommendation in multistakeholder settings needs to be approached differently from user-focused environments. In particular, we have found that formalizing and computing stakeholder utilities is a productive way to design and evaluate recommendation algorithms. Ultimately, the system owner is the one whose utility should be maximized: if there is some outcome valued by the recommender system operator, it should be included in the calculation of system utility. 

The system inevitably has objectives that are a function of the utilities of the other stakeholders. Multisided platforms thrive when they can attract and retain critical masses of participants on all sides of the market. In our employment example, if a job seeker does not find the system's recommendations valuable, he or she may ignore this aspect of the system or may migrate to a competing platform. The same is true of providers; a company may choose other platforms on which to promote its job openings if a given site does not present its ads as recommendations or does not deliver acceptable candidates.

System utilities are highly domain-specific: tied to particular business models and types of transactions that they facilitate. If there is some monetary transaction facilitated by the platform, the system will usually get a share. The system will also have some utility associated with customer satisfaction, and some portion of that can be attributed to providing good recommendations. In domains subject to legal regulation, such as employment and housing, there will be value associated with compliance with anti-discrimination statutes. There may also be a (difficult to quantify) utility associated with an organization's social mission that may also value fair outcomes. All of these factors will govern how the platform values the different trade-offs associated with making recommendations.



\subsubsection{Multisided fairness}
Recommendation processes within multisided platforms can give rise to questions of multisided fairness. Namely, there may be fairness-related criteria at play on more than one side of a transaction, and therefore the transaction cannot be evaluated simply on the basis of the results that accrue to one side. There are three classes of systems, distinguished by the fairness issues that arise relative to these groups: consumers (C-fairness), providers (P-fairness), and both (CP-fairness).

\subsubsection{C-fairness}

A recommender system distinguished by C-fairness is one that must take into account the disparate impact of recommendation on protected classes of recommendation consumers. In the motivating example from~\cite{fairness}, a credit card company is recommending consumer credit offers. There are no producer-side fairness issues since the products are all coming from the same bank. 

Multistakeholder considerations do not arise in systems of this type. A number of designs could be proposed. One option that we explore in this paper is to design a recommender system following the approach of \cite{zemel2013learning} in generating fair classification. We generate neighborhoods for collaborative recommendations in such a way to have balanced representation of the opinions across groups. 

\subsubsection{P-fairness}

A system requiring P-fairness is one in which fairness needs to be preserved for the providers only. A good example of this kind of system is Kiva.org, an on-line micro-finance site. Kiva aggregates loan requests from field partners around the world who lend small amounts of money to entrepreneurs in their local communities. The loans are funded interest-free by Kiva's members, largely in the United States. Kiva does not currently offer a personalized recommendation function, but if it did, one can imagine a goal of the organization would be to preserve fair distribution of capital across its different regions in the face of well-known biases of users~\cite{lee2014fairness}. Consumers of the recommendations are essentially donors and do not receive any direct benefit from the system, so there are no fairness considerations on the consumer side. 

P-fairness may also be a consideration where there is interest in ensuring market diversity and avoiding monopoly domination. For example, in the on-line craft marketplace Etsy\footnote{www.etsy.com}, the system may wish to ensure that new entrants to the market get a reasonable share of recommendations even though they will have had fewer shoppers than established vendors. This type of fairness may not be mandated by law, but is rooted instead in the platform's business model.

There are complexities in P-fairness systems that do not arise in the C-fairness case. In particular, the producers in the P-fairness case are passive; they do not seek out recommendation opportunities but rather must wait for users to come to the system and request recommendations. Consider the employment case discussed above. We would like it to be the case that jobs at minority-owned businesses are recommended to highly-qualified candidates at the same rate that jobs at other types of businesses. The opportunity for a given minority-owned business to be recommended to an appropriate candidate may arrive only rarely and must be recognized as such. As with the C-fairness case, we will want to bound the loss of personalization that accompanies any promotion of protected providers. 

There is considerable research in the area of diversity-aware recommendation~\cite{Vargas:2011:RRN:2043932.2043955,adomavicius2012improving}. Essentially, these systems treat recommendation as a multi-objective optimization problem where the goal is to maintain a certain level of accuracy, while also ensuring that recommendation lists are diverse with respect to some representation of item content. These techniques can be re-purposed for P-fairness recommendation by treating the items from the protected group as a different class and then optimizing for diverse recommendations relative to this definition.

Note, however, that this type of solution does not guarantee that any given item is recommended fairly, only that recommendation lists have the requisite level of diversity. This distinction is known as list diversity vs catalog coverage in the recommendation literature and as individual vs. group fairness in fairness-aware classification~\cite{fairness}. List diversity can be achieved by recommending the same ``diverse'' items to everyone, without necessarily providing a fair outcome for the whole set of providers. In this work, we are using metrics that measure group fairness, but we will extend these results to individual fairness measures in future work.
