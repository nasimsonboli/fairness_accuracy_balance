This chapter covers literature that is relevant to the dissertation as a whole, and will frame each study's contributions broadly in order to synthesize across empirical work in \autoref{ch:synth}. Each individual study chapter (\autoref{ch:dbd} through \autoref{ch:wkshp}) includes a more specific literature review customized for each study.

\section{Complexities of Data Work in Nonprofit Organizations}
Research on data use within nonprofit organizations explains that diverse stakeholders (i.e. experts, volunteers, funders) complicate, but also facilitate data analysis, given a lack of expertise or capacity in-house, introducing complicated power dynamics along the way \citep{Voida2011Homebrew, Erete2016Storytelling, LeDantec2010Boundaries, Voida2017Currencies, Verma2016DrillDown}. Other research has shown that data often does not accurately represent an organization’s work in terms of format or content \citep{Verma2016DrillDown, Benjamin2012FrontOut, Benjamin2015Agency, Marshall2016Accountable}. This may at least partially be caused by the practice of more often using data for accountability purposes—mostly reporting to funders—than for making day-to-day operational decisions \citep{LeDantec2008Trenches, LeDantec2010Boundaries, Stoll2010Interorg, Voida2017Currencies}.

The dual use of data for accountability and operations has caused some organizations to perform double (or more) data entry—entering data once into a system for accountability purposes, while entering data into another system for operational use \citep{LeDantec2008Trenches}. Expertise around information technology and database technology capabilities varies greatly among organizations \citep{LeDantec2008Trenches, Stoll2010Interorg, Benjamin2018Policy}. When expertise around database technology falls short of organizational need, which is often the case, staff and volunteers often adapt what technology they have to partially meet their needs, resulting in hybrid locally-appropriated systems \citep{Voida2011Homebrew, Verma2016DrillDown, Stoll2010Interorg, Voida2017Currencies}.

Despite the challenges discussed so far, nonprofit staff sometimes do use data for decision making, however, the often rigid tools they have at their disposal are often not flexible enough to capture the clients’---and in fact the nonprofit’s own---circumstances. \citet{Harmon2017Fictions} determined that these IT systems place unrealistic expectations about how social good is accomplished, how effective programs can be, and to whom organizations should be accountable.

Where human service organizations are collecting data to make decisions, staff may use digital or paper forms as decision support tools to ensure decisions are made in the same way across staff and are based on evidence instead of a staff member’s intuition \citep{Schwalbe2004HS}. Despite this intended goal, \cite{Schwalbe2004HS}'s review of research on child welfare workers and mental health professionals found that their use of forms for making and recording decisions was inconsistent, and there was often disagreement among case managers. Research reviewed by \cite{Schwalbe2004HS} suggests that there at least three barriers to use of these decision support tools: (1) staff may intentionally modify data entered into the tool so that it outputs a decision that matches their assessment of the situation, (2) the underlying logic of the tool may be questioned, and (3) tools are dismissed in favor of qualitative accounts by staff.

The phenomena of ill-fitting data schemas, however, is hardly unique to human services (see \cite{Bowker2000Sorting} for an example of inconsistencies among doctors coding causes of death). Looking beyond client-level decision support tools, \cite{Benjamin2012FrontOut} found that common outcome measurement guides provided to nonprofit organizations advocate for metrics that align with the way that a program was designed to work, not the way it actually works in day-to-day practice. Therefore, when it comes to making both client-level and programmatic-level decisions, the data used within decision frameworks fall short of connecting on-the-ground program reality with representations of the local lived experience.

The social issues in which organizations intervene are complex. What it means to “do good” can be ambiguous and multiply interpreted, and as \cite{Voida2014SharedValues} showed, this multiple interpretability precipitates challenges when a particular logic of “doing good” is codified in one way in an information system, and that logic is inconsistent with the logic employed by social service workers. The “logics of the system” \citep{Voida2014SharedValues} are codified in extensions of the human services information system through decision support forms or a social program’s logic model---a formal document that outlines the way that a program intends to change something about society. \cite{Voida2014SharedValues} observe that while both the system and the implementers have shared values, such as increasing access to services for clients, “mismatches in the way values are understood and put into practice can create significant tensions.”

\section{Data Doubles and their Performativity}
Haggerty and Ericson introduced the construct of a “data double” as the digital profile of an individual that can be “transported to centralized locations to be reassembled and combined in ways that serve institutional agendas” \citep{Haggerty2006New}. Writing about the role of surveillance in society, Haggerty and Ericson observe that surveillance technologies do not monitor people as a comprehensive and static set of data points, but rather as a series of information flows, which they point out are subject to “pre-established classificatory criteria” \citep{Haggerty2006New}. 

\cite{Raley2013Dataveillance} expands on the data double concept by applying it to internet browser activity, pointing out that cookies allow advertising companies to assemble streams of data that are comprised of “flecks of identity” \citep{Fuller2005Media} to develop data doubles that could be used to personalize the targeting of ads. In the construction of a data double to represent the human being “[d]ata is in this respect performative: the composition of flecks and bits of data into a profile of [the individual], the re-grounding of abstract data in the targeting of an actual life, will have the effect of producing that life, that body, as [the profile]” \citep{Raley2013Dataveillance}.

The performativity of data has been studied in other contexts beyond the internet, such as rivers \citep{Ribes2013DataBite}, flora and fauna \citep{Bowker2000Bio}, medical metrics \citep{Pine2015Politics}, and food \citep{Voida2017Currencies}. \cite{Bowker2000Bio} observed that “what is not classified gets rendered invisible,” and items that are less charismatic are often not classified and therefore rendered invisible. For example, \cite{Bowker2000Sorting} argue that tropical diseases have largely been rendered invisible because they have not been included in the coding system used across the medical domain. As databases are structured around medical codes that exclude tropical diseases, the scientific data-driven world where these diseases are not acknowledged come to be “ever more readily recognized as ‘the real world’...” \citep{Bowker2000Bio}.

Within the nonprofit domain, \cite{Voida2017Currencies} argue that the database systems used by organizations and tuned to collecting particular data based on external stakeholder needs become performative by shaping “the future of the organizations and institutions that we rely on to promote the public good and remedy social injustice.” 

\section{Rational and Formal Logics}
The driving force behind the scientific charity movement---a systematized approach to social issues that began in the early 1900’s---stemmed from a desire to have large-scale social impact through scientifically backed top-down social engineering \citep{Sealander2003Curing}. The movement was particularly effective at influencing medicine and education, for example, in encouraging the adoption of corporate accounting and its associated logic of systematization \citep{Sealander2003Curing}. To support this approach, foundations that advocated for scientific charity encouraged higher education to grow (at that time) the relatively new area of social science to establish methods and metrics for deconstructing and studying all components of society \citep{Sealander2003Curing}. At the same time that the scientific charity movement was gaining momentum, this systematized approach also touched the business and government sectors. In the business world, it was through the scientific management movement, and in the government, it was through the creation of institutions like New York City's Bureau of Efficiency \citep{Brest2020Outcomes}.

A few decades later, the 1950’s and 1960’s saw the rise of the “high modernist” form of modernity, which encapsulates the belief in “supreme self-confidence about continued linear progress” \citep{Scott1998Seeing} through science and technology. This approach to progress is achieved through the application of scientific knowledge to “the rational design of social order, the growing satisfaction of human needs, and, not least, an increasing control over nature (including human nature) commensurate with scientific understanding of natural laws” \citep{Scott1998Seeing}. Not very far off in the distance, outcome measurement in nonprofit organizations as we understand it today---the process of recording and tracking an organization’s progress against their stated mission---began ramping up in the 1990’s in response to funder requirements \citep{Benjamin2012FrontOut,Brest2020Outcomes}. These funder requirements were shaped by over a century of a growing emphasis on evaluation by foundations like the Rockefeller Foundation and the Ford Foundation, which coincided with a similar emphasis in other sectors of society such as business and government \citep{Brest2020Outcomes}.

Today, the philanthropic movement called ‘effective altruism’ argues that “high-quality evidence and careful reasoning” are necessary in the social sector, and that “many people can...have a tremendous positive impact on the world, if they choose [social interventions] wisely” \citep{EffectiveAltruismIntro}. However, effective altruists are only a small portion of donors. Speaking more broadly, philanthropic research indicates that many foundations are “optimistic about evolutionary social change occurring through rational planning by so-called ‘politically neutral’ experts” \citep{Arnove2007Revisiting}.

This rational and evidence-based momentum plays out within nonprofit organizations as a strong push to evaluate their impact, which is often imagined by nonprofit employees as being possible through standardized quantitative metrics, whereas these employees believe that qualitative data only helps people “connect” with individual’s stories \citep{Verma2016DrillDown}.

This rational and scientific approach to work is viewed by \cite{Berg1997OfForms} as limiting the human element needed to do the work, explaining that “formal tools inevitably function in a rigid, impoverished way, thus de-skilling and dehumanizing the work of those who are caught in its cold, instrumental rationality.” Though, as \cite{Bowker2000Sorting} point out: “People do not really follow formal rules; they make up their own. They tailor rigid computer systems to their everyday working needs.” While the US nonprofit sector has a much longer history that predates the founding of the nation (see \citet{Hall2006History}), in summarizing its history starting in the early 1900's---as described above--- the nonprofit sector is deeply rooted in historical contexts that encourage social progress through rational and evidence-based approaches, though there are questions as to whether such an approach is well suited for socially impactful work.

\section{Map and Terrain}
The relationship between the representation of the local experience and its aggregate is a theme that weaves through numerous lines of work. Early studies of the French census, for example, highlighted that “no single set of categories could be adequate” and instead asked the local census authorities to supplement the national census categories with locally defined categories \citep{Porter1995Trust}.

The invention of scientific forestry in late eighteenth-century Prussia and Saxony developed the differentiation between---quite literally---the forest and the trees. These local and more global orientations connected to early modern European state efforts in aggregating the local terrain in the form of cadastral maps which would allow for easier management (and taxation) of territories without requiring intimate local knowledge. Conceptually, maps or aggregations of a geographic area are valued for their “abstraction and universality...[and] are designed to make the local situation legible to an outsider” \citep{Scott1998Seeing}. These maps, as Scott argues, present a simplified view “far more static and schematic than the actual social phenomena they presume to typify.”

Naive formalists believe that the map captures all the essential elements to base decision-making on, while opponents of that view express concerns about the level of interpretation and loss of detail in mapping the territory \citep{Berg1997OfForms}. However, the way people really use these tools falls somewhere between the two perspectives.

Just as decision-making occurs across both levels, the connection between the two worldviews remains extremely strong. The entity that is able to more seamlessly move between both worldviews is the data double, which can be examined as both a tree and, after aggregation, as a forest. This is only possible when a link is maintained between both levels. For example, in looking at water samples that are collected from local streams and analyzed to derive a number of metrics which can then be entered into a database, \cite{Ribes2013DataBite} point out that the stream’s data double relies on the linking back to the physical, local stream.

While the stream’s local identification number that maintains this link is simple and mundane (procedural metadata), if the ID becomes detached from the data double, the entire system breaks down because the sample cannot be associated with the source stream—in fact, relational databases work the same way. Connecting this observation more directly to the map/terrain metaphor, we can see the way that the terrain remains linked, yet becomes abstracted through the division of land west of the Ohio River into squares as influenced by Enlightenment rationalism. Regardless of the local terrain at those boundaries, the terrain is made legible at the map level thereby allowing it to be “registered and titled from a distance by someone who possessed virtually no local knowledge” \citep{Scott1998Seeing}.

The legibility of a local context at the map level opens it up to intrusion from outsiders, allowing them to navigate the terrain without the input of insiders, thereby reducing the privilege of local citizen’s knowledge \citep{Scott1998Seeing}. This form of surveillance that is possible at all levels, based on data doubles, is only possible “...provided a set of different databases are networked and provided that they share the same means of establishing individual identification” \citep{Raley2013Dataveillance}. 

Just as decisions about land management are made at the terrain and map levels, human services stakeholders also make decisions at the individual and aggregate levels. Within the literature on nonprofit data usage, \cite{LeDantec2008Trenches} break down the public sector into various “scales of influence and accountability” (referred to here as levels) that include local service providers, municipal entities, state, and national levels. They explain that “there are three characteristics that make scale crossing endemic in the public sector: upward accountability, lateral cooperation, and internal work practice.” Upward accountability includes lower levels reporting up to government, regulatory, and funding agencies; while lateral cooperation involves working with other organizations to provide a community service. “Finally, internal work practice comes to bear as the same information systems used to collect data (for upward accountabilities) and coordinate care (for lateral cooperation) are intended to support day-to-day case management” \citep{LeDantec2008Trenches}.

While the abstraction of local context for manipulation by higher levels of government and policymakers appears potentially problematic for the individual clients and citizens living within that local context, \cite{Raley2013Dataveillance} points out that “happily, sheer impracticality means that data systems can never function as perfectly as our dystopian imaginations might suspect. The errors inherent within a catalog mailing list, one of the more basic data sets, indicates how unstable that data can be: any given population is a massive moving target...”

\cite{Scott1998Seeing} also explains there are drawbacks to both local and global perspectives in that reality cannot be fully grasped at either level. The person on the ground cannot understand a city’s overall design as well as someone in a helicopter can, however the person up in the air also cannot understand the details of the street corner experience. \cite{Berg1997OfForms} argues that the formal and informal, map and terrain, do not exist as binary options, instead power is realized in the “interrelation” of the formal and informal. Additionally, \cite{Bowker2000Bio} points out that while a single database might be theoretically possible, many fields including medicine, have been attempting to do this for many years and have thus far not succeeded. Given this, \cite{Bowker2000Bio} suggests that instead we should “...look at the machines that produce local orderings and alignments of datasets (oligopticons),” referring to \cite{Latour2005ANT}'s oligopticon that is meant to be the inverse of the panopticon, where instead of seeing all, the guard sees very little, “but what they see, they see it well” facilitating “sturdy but extremely narrow views of the (connected) whole.”