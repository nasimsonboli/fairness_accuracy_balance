\chapter{Fairness}
\label{fairness}


\section{Fairness \& Justice}
% There are two types of assumptions behind fairness-aware systems \cite{friedler-impossibility-2021}: (1) \textit{What you see is what you get (WYSIWYG)}, and (2) \textit{We're all equal}. Having a WYSIWYG assumption in a system means that the observations of features do not encode bias, therefore the system uses that unbiased information to make decisions about individuals in a way that similar individuals are treated similarly. WYSISYG seeks to achieve fairness for individuals rather than a group of users. However, the second assumption (WAE) acknowledges that the biases in the society are reflected in data which lead to disparate treatment of different groups with different sensitive features. Therefore WAE supports group fairness.

\section{Fairness in Machine Learning}


\section{Fairness in Recommender Systems}
% \subsection{Calibration?}

% There has been considerable research in the area of diversity-aware recommendation~\cite{Vargas:2011:RRN:2043932.2043955,adomavicius2012improving} where these systems treat recommendation as a multi-objective optimization problem where the goal is to maintain a certain level of accuracy, while also ensuring that recommendation lists are diverse with respect to some representation of item content. These techniques can be re-purposed for P-fairness recommendation by treating the items from the protected group as a different class and then optimizing for diverse recommendations relative to this definition.

\section{Dynamic fairness}

    It is important to consider fairness in recommender systems as a dynamic property, because, recommender systems operate in dynamic environments where it continuously makes new decisions based on the recent changes in the system such as: gaining or losing consumers, gaining or losing (item) providers and the different items that they provide based on the current needs of the market, seasonal changes, and etc.

    Some of these dynamic changes can introduce biases into a system that might have been "de-biased" recently or they can even re-enforce the existing biases. 
    
    \textbf{Positive feedback loops} are among the most destructive dynamic changes that happen in recommender systems \cite{o2016weapons} that are caused by presentation bias.
    
    This type of bias occurs because users are more likely to interact with items that the system presents to them. 
    
    The first issue here is the item selection by the recommender system here and whether it will contribute to unfairness to any of the stakeholders. We have addressed how to mitigate this issue in Chapter \ref{fairness} in the pre-processing section \todo[inline]{preprocessing section}. 
    
    The second issue is that presentation bias, can lead to a form of positive feedback loop, in which presented items gain more popularity since they are more likely to be interacted with. This leads to greater bias towards presenting the items when the popular items are promoted more at the cost of other items. Presentation bias and the created feedback loop not only magnifies the initial differences between items' popularity but also it makes it hard for new providers to attract the attention of users to their products/items in a system with this type of bias.
    
    For example, in the microlending case, if the system doesn't recommend loans from a specific geographical region because on average the requested loans from this region are risky (their borrowers are less likely to return the loan), not only the current good borrowers (the borrowers who are more likely to return the loan) from that region are affected, but also the future good borrowers. This positive feedback loop re-enforces over time until that region is completely ignored by the system.
    
    \cite{Chaney2018Homo} shows that in an iterative environment, recommender systems increase the inequity in item exposure and homogenize the recommendation lists across different users which is undesirable. \cite{pmlr-v80-hashimoto18a} shows in a systems where users engage with it iteratively, machine learning models that are trained on average loss, suffer from representation disparity (presentation bias) which over time these systems lose their minority groups. To mitigate this bias, they develop an approach based on distributionally robust optimization (DRO) that achieves a more balanced performance between the dominant subgroups and minority ones. \cite{NEURIPS2019_7690dd4d} study this idea in a sequential framework and show that representation disparity gets worse under the natural user dynamics model, resulting in some groups diminishing entirely from the sample pool in the long run. to mitigate feedback loops in the context of predictive policing, \cite{pmlr-v81-ensign18a} develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system so the feedback loop does not occur, leading to better decision-making systems.
    
    To detect and evaluate dynamic biases in systems, It is necessary to have dynamic evaluation methodologies as well.
    
    \textbf{Dynamic Evaluation} methodologies therefore, are required to evaluate such biases in a dynamic system.
    
    Here instead of using batch training and then batch testing, we need to incorporate a cycle which starts with user arrival, training the model, recommendation generation, and then collection of user feedback on those recommendations and continues with periodically re-training the system and generating new sets of recommendations at every iteration.
    
    This evaluation methodology has become common in recommendation approaches that use reinforcement learning where the environment is dynamic \cite{Lihong2010bandit,Zheng2018DRN}. Evaluation of this cycle can also be simulated in off-line settings.  
    
    To appropriately evaluate the fairness properties of recommender systems, in practice we might need to look at unfairness in a particular time interval. And then we might have to consider the evolution of unfairness mitigation and/or aggravation through multiple evaluation cycle.
    
    I have made use of this evaluation methodology in reranking \cite{sonboli2020dynm}. I propose an adaptive recommendation approach to address the problem of multidimensional fairness using probabilistic social choice to control feedback loops and adjust the system over time. I compute deviations from fairness in a particular time window and then compensate it over the next batch of generated recommendations by adjusting the fairness objective. For more details please refer to Chapter \ref{fairness_postproc}.

    \cite{biega2018equity} uses time in their re-ranking as well. They consider past rankings so that the ranking at a particular time $t$, improves the aggregate fairness of the system through time $t$.
    
    
    In the following chapter, I will introduce baseline recommendations, data sets, the approached we adopted to define the protected group(s) and the unprotected group(s) and all the details of the experimental design to increase reproducibility.