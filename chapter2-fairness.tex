\chapter{Fairness}
\label{ch:fairness}

In recent years, the increased adoption of algorithmic systems into many realms of society has raised concerns about the harms and unfairness that such systems cause. Among these concerns, the issue of fairness in computing systems, in the decision support context, especially the systems that rely on machine learning and statistical tools, has gained a significant attention\cite{mitchell2021algorithmic}. This is particularly evident where high stakes decisions are made, ones that have significant real-world impact on individuals’ lives and livelihoods such as healthcare, public policy, and law enforcement.

The discussion of fairness, justice, and equity is only one of the social aspects investigated and criticized in machine learning systems. Such problems have gained the attention of a multidisciplinary community from computer scientists in different fields (e.g., artificial intelligence, machine learning, etc. to social scientists and legal scholars that have started to study ``Fairness, Accountability, and Transparency'', and ``AI ethics'' in their fields.

This chapter provides a short overview of algorithmic fairness for classical machine learning problems and recommender systems, including its fundamental concepts, definitions, potential sources of unfairness, and different families of solutions that can be applied to reduce fairness in machine learning systems. For a deeper discussion on these topics, please refer to the work of \cite{mitchell2021algorithmic,barocas2016big,barocas2018fairness}.

\section{Fairness and Justice}
\label{sec:fair_&_justice}
    
    One of the most fundamental concepts in the pursuit of fairness both in the society and computing systems is an axiom traced to Aristotle: \textit{Equals should be treated equally, and unequals unequally, in proportion to relevant similarities and differences.} This axiom establishes a rule that defines similarities according to cases’ objective characteristics as opposed to the subjective perceptions of the judge, the maxim forces decisions to be consistent with this rule so that that equal users are treated equally before the law \cite{Gosepath2011equality}. Although ``treating unequals unequally'' is a vague sentence. 
    
    This issue is addressed in the scholarship on distributive justice by recognizing four classic principles: \textit{exogenous rights}, \textit{compensation}, \textit{reward}, and  \textit{fitness} discussed in Moulin \cite{Moulin:FairDivision}. Exogenous rights are the rightful external claims that the system should satisfy such as equal shares in a property defined by a contract. Compensation recognizes that fairness needs extra consideration where the costs are not equal. Affirmative action in college admission or in hiring are examples of this principle. The principle of the reward demands to gain benefits in proportion to contributions. For example, employees get increased bonuses according to their contributions to that organization.
    
    The argument on fitness demands that resources go to those most fit to use, or in other words, whoever makes the best use of resources for the benefit of all. This idea is ideologically close to the concept of utilitarianism which favors a distribution that maximizes the overall utility (sum of the individual utilities), ignoring the needs of particular individuals. Fitness has a natural application in machine learning systems, specifically recommender systems as we deliver information (recommendation lists) to individuals based on their utility and fitness to users.
    
    Anti-discrimination law in the U.S. legal theory has given rise to such essential concepts as \textit{disparate impact}. Anti-discrimination law ensures that people are not denied certain benefits like housing, education, work, etc., based on \textit{protected characteristics} such as race, color, religion, gender, disability, age, or in many jurisdictions, sexual orientation. Using the disparate impact standard, a discriminatory action can be challenged legally based on the disproportionate adverse impact on a protected group, without showing the intent to discriminate.
    
    Crenshaw (2018) \cite{clark2018demarginalizing} discusses that this legal framework is limited as it only focuses on discrimination on the basis of individual protected characteristics while people who suffer from the combination of protected characteristics have a hard time proving their case.
    
    Fairness and discrimination have been a topic of discussion for decades in other communities, such as educational testing, which has a large body of literature on fairness in their domain \cite{Hutchinson2019history}. Friedman et al. (1996) \cite{Friedman1996Bias} provides one of the earlier discussions on bias in computer systems and how technical decisions may result in biased effects when computer systems are used in the social context. 
    
    In the last decade, fairness in machine learning has been a topic of interest in the research community and is being actively expanded in this field. In the following section, we will describe an introduction to this literature.

\section{Fairness in Machine Learning}
\label{sec:fair_ml}
    
    % \todo[inline]{more on the fairness concept}
    Fairness is a complex concept, and it does not encapsulate one definition. Algorithmic fairness as well is not one goal but rather a spectrum of different equity concerns. To understand algorithmic unfairness, we need to initially recognize the type of unfairness that happens in a system and understand the stakeholders (consumers, providers, etc.) affected by the harms caused by this unfairness. Only after taking these steps can we design methods to assess a system's unfairness and finally design approaches to diminish unfairness in that system.
    
    Although researchers and practitioners should bear in mind that machine learning might not always provide a solution. Formulating fairness definitions in mathematical forms without considering the full meaning of such a social concept might not resolve the problem. Selbst et al. (2019) \cite{selbst2019fairness} refers to this issue as \textit{formalism trap} and identifies a number of other ``abstraction traps'' in fairness research. In addition to these traps, one should understand that fairness, in its foundation, is contextual and contestable. Additionally, incompatibility of different fairness notions \cite{friedler-impossibility-2021} implies that universal fairness is not an achievable (and even meaningful) concept.
    
    Considering the discussed limitations, researchers identify specific ways in which they \textbf{can} determine a system's (mathematically and reasonably definable) unfairness and try to develop tools to measure and mitigate unfairness in different domains. Much of the work in algorithmic fairness has been focused on classification, with a myriad of definitions that has been proposed. The key definitions are explained in \cite{mitchell2021algorithmic,barocas2016big,barocas2018fairness}.
    
    To clarify unfairness definitions and apply the appropriate solutions to mitigate them, we need to break them down into different categories. And this categorization (along with their assigned names) is based on the type of harm (distributional or representational) that happens in a system, the location in the pipeline that it happens, and whether this harm is affecting individuals or groups of users and how unfairness is measured. In the following paragraphs, I will discuss these fairness definitions further.

    In the algorithmic fairness scholarship, sometimes the terms such as ``bias'', ``fairness'', ``discrimination'', ``inequity'', and ``injustice'', which were mostly borrowed from legal scholarship, are used interchangeably without considering their nuances. Therefore, different authors use these terms subjectively to some extent. Therefore it is important to understand the usage of these words from the perspective of different authors. Throughout this dissertation, I will specifically determine the type of unfairness for every research problem, but will use these words interchangeably as well.
    
    \subsection{Individual \& Group Fairness}
        
        When we assess the unfairness of a system, it is also important to identify whether individuals or groups of users are being affected. \textbf{(\romannum{1})~Individual fairness} is concerned with the similar treatment of similar users. This normative principle requires that like cases should be treated alike \cite{binns2020conflict} which is a fairness notion that goes back to Aristotle. Cynthia Dwork (2012) \cite{Dwork2012individual} introduces the concept of individual fairness which posits that similar individuals with respect to the task at hand should be treated similarly or in other words should have similar probabilities of positive classification outcomes. One of the limitations of this method is the choice of similarity metric to compare individuals and whether this metric is unbiased. This method also doesn't place any requirement on the treatment of dissimilar individuals. Additionally, this method will not be able to violate individual fairness for highly dissimilar individuals. Another individual fairness definition was proposed by \cite{pmlr-v70-kearns17a} for the problem of candidate set selection from diverse incomparable source sets. Choosing candidates for a research position from a diverse research communities with incomparable research metrics (e.g. citation rates are different in different research communities) is an example of this problem. Meritocratic fairness requires that less qualified candidates are probabilistically almost never chosen over the more qualified candidates.
        
        \textbf{(\romannum{2})~Group fairness} is concerned with ensuring different groups have comparable experiences with the system. These groups are formed based on sensitive attributes like race, gender, age, etc. The sensitive attribute definition is usually rooted in anti-discrimination laws\cite{barocas2016big}. This notion of fairness tries to ensure that algorithms don't impact the members of the protected group more adversely and disproportionately. Group-based methods of fairness has helped build the most prevalent structures to achieve and assess fairness ~\cite{zemel2013learning,kamishima2012fairness,kamiran2010discrimination,zhang2017anti}.
        
        \textbf{(\romannum{3})~WYSIWIG and WAE}
        
        There are two types of axiomatic assumptions behind fairness definitions, and metrics \cite{friedler-impossibility-2021}: (1) \textit{What you see is what you get (WYSIWYG)}, and (2) \textit{We're all equal}. Having a WYSIWYG assumption in a system means that the observations of features do not encode bias. Therefore the system uses that unbiased information to make decisions about individuals in a way that similar individuals are treated similarly. WYSISYG seeks to achieve fairness for individuals rather than a group of users. Although if the data is biased, this assumption cannot guarantee that the individuals are similar and therefore we will not treat individuals similarly anymore. Therefore this is an strong assumption. 
        
        The second assumption (WAE) considers that all groups are fundamentally similar with respect to a task. So any systematic discrepancy is the result of discrimination and should be corrected. This assumption acknowledges that the biases in the society are reflected in data which leads to disparate treatment of different groups with different sensitive features. Some researchers suggest to consider WAE or WYSIWYG as priors instead of axioms to have more flexible constraints.
        
        % WAE can be applied in both individual fairness and group fairness. Because WAE can be interpreted as "members of different groups are the same" or "members of the different groups should be treated the same with respect to a specific task". 
        
        % Group fairness can also be derived from WYSIWIG or WAE assumption.
        
     
    \subsection{Harm}
        Algorithmic fairness in machine learning often addresses two significant issues: (1) distributional harms and (2) representational harms\cite{crawford2017trouble}. Distributional harm occurs when the positive or negative effects, outcomes, or resources are not distributed fairly among its subjects. Therefore someone is denied a benefit unfairly. While in representational harms, groups or individuals are not represented fairly by and in the system. The literature in algorithmic fairness mostly considers distributional harms and proposes to increase representational fairness to reduce it.
    
    \subsection{Disparate Treatment and Disparate Impact}
        
        \textbf{Disparate Treatment} occurs when members of different groups are explicitly being treated differently \cite{barocas2016big}. This discrimination usually happens because the model has access to the sensitive attribute(s). Therefore omitting or hiding this information from the model might solve this problem.
        Dwork et al. \cite{Dwork2012individual} discuss that ``fairness by unawareness'' that is achieving fairness through ignoring the protected group status does not work because group identity usually correlates with other variables \cite{Feldman2015} therefore allowing the model to pick up the differences through these variables. They suggest using individual fairness to address this issue through ``similarity with respect to task'', which may compensate for group differences. WAE-based methods such as statistical parity, or error parity try to remedy this issue by requiring that different groups experience positive decisions or erroneous outcomes at the same rate.
        
        \textbf{Disparate Impact} \cite{Feldman2015} occurs when the impact of the system's decisions is unfairly different for different groups. In machine learning, this idea is formalized through the concept of \textbf{statistical parity}. The notion of Statistical or Demographic Parity requires that two groups with a different sensitive group have the same chance of getting a positive result. This notion has been discussed under other different names like independence \cite{barocas2018fairness}. Disparate impact is used as a test to measure unfairness in anti-classification law\cite{corbett2018measure} which is usually operationalized via the ``four-fifth rule''. This rule requires that the protected group pass rate should not be less than four-fifth (80\%) of the majority group pass rate. Disparate impact has a WAE (we are all equal) assumption, meaning fairness is achieved if all the groups (with similar characteristics) are treated similarly. This measure is used to ensure a ``fair'' representation of different groups in different tasks such as ranking\cite{singh2018fairness,zehlike2017fa,yang2017measuring}, and recommendation\cite{mehtora2018towards,ekstrand2018exploring}.
    
    
    \subsection{Performance Parity}
        Performance parity is an error-based construct that is used to group fairness. This family of metrics makes a WYSIWIG (what you see is what you get) assumption for the outcome. They assume that the ground truth (recorded outcome in the data) is unbiased. The goal is to use it as a reference point to ensure that groups are not mistreated (experiencing a different outcome). Below are some of the most commonly used metrics to address fairness in classification.
        
        \textbf{Error parity}, sometimes called \textbf{disparate mistreatment} seeks to achieve equality of misclassification rates, that is the equality of false-negative rates (FNR) \cite{zafar2017fairness}. In other words, error parity ensures that different groups do not experience erroneous decisions at different rates, conditioned on their true outcome.
        
        \textbf{Equality of opportunity} or \textbf{recall parity} ensures the equality of true positive rates \cite{hardt2016equality}. In other words, the positive classification rates should be independent of the protected attribute given the true label. In this case, members of different groups are equally likely to receive a favorable positive decision conditioned on the positive outcome.
        
        \textbf{equalized odds} \cite{hardt2016equality} ensures the equality of both true positive (TPR) and false positive rates (FPR). Equality of positive predictive values, and \textbf{calibration} \cite{Kleinberg:InherentTrade} which requires equality of well-calibrated scores for all the groups, are other group fairness objectives.
        
        The previously described metrics are not the only metrics for this purpose. We can define error parity metrics on any margin of the confusion matrix \cite{mitchell2021algorithmic}. However, it is worth noting that, based on Chouldechova-Kleinberg theorem \cite{chouldechova2017fair,Kleinberg:InherentTrade}, we cannot equalize more than two error parity metrics unless the underlying base rates are equal or the classifier is perfect. There are other fundamental tradeoffs between other error party metrics as well, for example between calibration and error-based parity metrics \cite{pleiss2017fairness}. Aside from that, the WYSIWIG assumption behind these metrics is a strong assumption about the accuracy, since it assumes the ground truth is unbiased. However, in reality the recorded data can be biased. Error Parity has also been studied as error parity in recommendation algorithms\cite{ekstrand2018all,yao_huang_fatml-2017}.

    
    \subsection{Anti-classification and Anti-subordination}
        
        The motivation of fairness constructs also categorizes fairness definitions into two groups: anti-classification and anti-subordination. U.S. anti-discrimination law is rooted in anti-classification ideas, which require that a protected group membership such as race should not play a role in the decision-making process. This notion requires lessening the effect of disparate treatment.
        
        The theory of anti-subordination says that this rule is not very effective because the past discrimination carries forward in time. Therefore there is a need to actively work to reverse the effects of historical discriminatory patterns in the decision-making processes \cite{barocas2016big}.
    
    Unfairness can happen in any part of a system's pipeline, such as the data collection, model, evaluation, etc. I will explain different sources of unfairness in the next section.

    \subsection{Sources of Unfairness}
        The harms that are caused by a system arise from some type of bias, where there is a discrepancy between the expected and existing observations or the outcomes. These biases can creep into any part of the decision support system: in the outside world, in the collected data, in the development of the models, their evaluation, or their application \cite{sureshframework2019}. Here we describe the sources of unfairness in different parts of a machine learning system's pipeline or the the its feedback loop. Unfairness may arise in any part of this process and may get mitigated, propagated or exacerbated or even re-formed in the same stage or the following stages.
        
        % \todo[inline]{create a plot or doodle and add and refer to it here.}
        
        \textbf{(\romannum{1})~Society} or the outside world may be unfair. This type of bias is usually historical and ongoing. Redlining in United States housing policies \cite{rothstein2017color} is an example of this type of bias, which prevented Black Americans from home ownership in wealthier neighborhoods; neighborhoods with better amenities such as parks or better schools that improve the quality of life and childhood development. Due to such actions by private-sector and government policies, Black residents were prohibited from accessing the same opportunities for wealth building which has lead to significant racial disparity in wealth through home ownership and even disparate quality of education.
        
        \textbf{(\romannum{2})~Data} is another source of bias. The data can get biased during the data collection process. In some cases, some people are more likely to respond to surveys or volunteer information. This type of bias is known as response or submission bias. Sampling strategies are used to collect a certain amount of data for a specific purpose might fail to collect a sample that is representative of the real data or the real world. This issue can contribute to representation bias. The previous issues may construct imbalanced data where the size of different users differ. In this case, the prediction/recommendation models may become more accurate for the groups with greater group sizes as they have more data while being more erroneous in predictions for smaller-sized groups.
        
        In the data collection process, sometimes the perspective of the people involved in the process can invite bias into the data, mainly if there is little engagement with the stakeholders (for whom we are providing a service) to understand their needs. These biases will go undetectable if there is not clear documentation of the perspectives and assumptions that went into its design \cite{Hutchinson2021Account}. Lack of documentation, becomes problematic particularly when such sensitive personal attributes such as gender or race are recorded. Categorization of such attributes like gender is usually adapted from administrative data collection which might not be inclusive of all the differences. Therefore, their categorization and how they are recorded vary over time as the social and cultural constructs evolve \cite{Hanna2020CriticalRace}. Therefore it is important to interpret the data according to its time and also according to the cultural and social contexts to avoid the previous biases and to avoid unfairly disregarding local knowledge and perspectives.
        
        Mitchell \cite{mitchell2021algorithmic} differentiates between societal bias and systematic or statistical bias. Societal bias is the deviation between the \textit{world as it should and could be} and \textit{the world as it is}. Redlining is an example of societal bias since without this bias, the world could be different with Black residents living in any neighborhood. Statistical bias results in systematic discrepancies between \textit{the world as is} and \textit{the world that the data represents}. For example, if the records of housing information differs from the actual housing, it can cause a systematic mismatch.
        
        \textbf{(\romannum{3})~Models} can also exacerbate the existing unfairness or introduce unfairness. Models can learn to discriminate based on the sensitive attributes with or without having these attributes in the data. Models can indirectly learn the sensitive attributes based on the correlations of other variables with the sensitive attributes or the behavioral patterns associated with those attributes. The objective functions may also include biased perspectives about what constitutes a \textit{good} model and result in biased outcomes.
        
        \textbf{(\romannum{4})~Evaluation} of machine learning algorithms is also bias prune. All of the issues with data that we discussed before (such as sampling strategies, group sizes, etc.) also apply to evaluation. The perspectives in the design of the evaluation metric and how these perspectives define success in the outcome can bias the outcomes. For example, if the evaluation metric is aggregate, it will hide the biases for the minority group. Or some metrics measure and reflect success on stakeholder while other experience bias. Therefore, the perspective of the decision makers involved in the design of the metric, the model, the data collection methods can bias the outcomes.
        
        \textit{(\romannum{5})~Human Responses} are another factor that biases the outcomes. Some machine learning systems are interactive, so they record human responses to their results and feed this data back to the system. Srivastava et al. (2019) \cite{Srivastava2019fairnessnotions} discusses how algorithms and humans might disagree in this case on the notions of fairness in a way that their response might even be the exact opposite of the fairness definitions. Green and Chen (2019) \cite{Green2019algointheloop} found that the racial disparity in human assessment of risk scores increased after they were provided with the risk scores that were produced by a racially fair algorithm. Besides algorithms, other factors may skew human responses which can skew the current outcomes or the outcomes in the next iterations. Unfairness as we described can happen in any stage of the machine learning systems. Each part needs different interventions to mitigate the biases. Although, the bias that is removed in one stage can be re-introduces in another stage of the pipeline.
        
    % \todo[inline]{should I have the table of different biases in ML that is in the book chapter?}
    
    % \subsection{Mitigation methods}
    % To mitigate algorithmic unfairness, interventions can be made at different steps of the processing pipeline. \cite{Friedler2019} provides a broad overview of these methods.
    \todo[inline]{A common section for fairness interventions for both ML and recsys}
    
    The algorithmic fairness literature has identified several different constructs for measuring and reducing fairness, but these constructs are not always mathematically or conceptually compatible. This could be due to differences in their assumptions about the data, ethical goals, or social contexts. Therefore it is crucial to define and assess a specific problem setting in light of its full sociotechnical context. To define fairness for recommender systems, in addition to the previous categorizations, we also need to account for the unique challenges that arises in recommender systems: personalization and the multi-stakeholder problem. I will discuss these issues discussed in the following section \ref{sec:fairness_recsys}.

\section{Fairness in Recommender Systems}
\label{sec:fairness_recsys}
\todo[inline]{missing}

% In addition to the group vs individual fairness distinction, recommender system fairness is also distinguished by the fact that fairness concerns may be formulated relative to multiple different stakeholders \cite{burke2017multisided,abdollahpouri2020multistakeholder}. In particular, we may be concerned about fairness towards consumers of recommendations (end-users) and providers of items being recommended. Consumer-side group fairness asks whether the system is fair to different groups of users: for example, male, female and non-binary job seekers getting recommendations of job listings. Provider-side group fairness asks whether the system is fair to different groups of item providers: for example, male, female and non-binary musical artists whose tracks are being recommended.

% \todo[inline]{do they belong under the fairness in recommender systems?}
\section{Consumer Fairness}
\label{fairness_cf}

Recommender systems exist to facilitate transactions between all the stakeholders such as consumers of recommendations, and the providers of the items that were recommended (content providers), etc. Many recommendation applications involve multiple stakeholders and therefore may give rise to fairness issues for more than one group of participants~\cite{burke_multisided_2017}. Among all these stakeholders consumers and providers have the highest stake. Here we  define consumer fairness and discuss the literature that has addresses the consumer unfairness problem.
% \todo[inline]{in recsys?}

Consumer fairness (C-fairness) is concerned with the ways recommender systems impacts consumers/users or subgroups of users and whether or not those impacts are fair and equitable. These impacts are specifically crucial for marginalized groups and are sometimes required by law. For example, it is required by law that job recruitment systems don't discriminate based on race, gender, age, and other demographic information.
% Therefore they all the subgroups of users are receiving quality recommendations not only certain groups. 

    \subsection{Individual Fairness}
    If the goal of a system for consumers is individual fairness, then the system needs to make sure that similar individuals have similar experiences or quality of service within the system. This similarity can be calculated based on any user characteristics that the system might have been discriminating against such as gender, location, user profile, etc. Previous literature has used the accuracy of the recommendation outcomes to compare the quality of the service that individual users are experiencing. 
    
    In offline experiments, typical evaluation measures such as nDCG or recall can be used to measure accuracy. Another evaluation approach is to measure the distribution of utility for all the users. (Hashimoto et al.,2018 \cite{hashimoto2018fairness}) develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. They try to control the discrepancy in accuracy across users by constraining the performance for all of the users within a particular error region. This method hasn't been utilized in recommender systems due to some reasons. This method assumes that users are all the same and thus we can ensure a minimum quality of service for all of them. Whereas in recommender systems users might not be considered equal due to some reasons. For example, we do not have enough data about the users who recently joined a system yet (cold-start users) and this lack of data on this group of users will lead to a worse accuracy for them. Therefore performance discrepancy between the cold-start users and the users with denser profiles becomes inevitable. Applying the previous method here means to lessen the quality of recommendations for users with denser profiles in order to achieve equal accuracy between these user groups which is undesirable.
    % \todo{write the names of all the papers you cited and put in parenthesis} 
    
    \subsection{Group Fairness}
    Group fairness in a system means to ensure that the system provides the same (or comparable) quality of service or utility to different consumer groups within the system. For example, in a job recommendation website, if, on average, women receive lower-paying job recommendations than the rest of the users, this could be considered unfair. One way of detecting any potential performance discrepancy between the consumer groups is to compute the utility that is used to evaluate system's general performance such as offline accuracy evaluation, or an online A/B test, and then break down that utility by consumer groups. In other words, we calculate the average utility per consumer group. Utility can be operationalized based on the general measures that assess system's general effectiveness including ranking accuracy measures such as nDCG, and ERR, click through rates (CTR), or any online or offline measures.
    
    Yao and Huang \cite{yao_huang_fatml-2017} discuss different kinds of error-based unfairness metrics for collaborative filtering that captures different types of disparate prediction errors for protected and unprotected groups. They both calculate the overall disparate errors and over- and under- estimations of predictions. Ekstrand et al. \cite{ekstrand2018all} disaggregated off-line top-N evaluation of several collaborative filtering algorithms (measured by nDCG) and compared the results for different user demographics. They found statistically significant differences in utility between gender and some age groups. This disparity in utility is similar to the disparate impact concept because we observe a discrepancy in the outcomes for different groups. Ekstrand et al. \cite{ekstrand2021fairness} discusses that since information access systems (including recommender systems) don't directly make a decision about the users based on their differences in a protected attribute, disparate impact can be translated to \textit{disparate effectiveness} instead where the system is more or less effective for different user groups. 
    
    \subsection{Accuracy-based Fairness}
    Different approaches have been offered to correct inequitable distributions of system utility. Yao and Huang \cite{yao2017beyond} introduce regularizers to mitigate discrepancies in rating prediction errors. Re-ranking recommendations to improve their fairness is also a post-processing approach that can be used for this goal. This method is mostly used to mitigate provider-side fairness, but there are examples of its usage for consumer fairness in Abdollahpouri \cite{abdollahpouri2020popularity}. In their research, user groups are defines based on their level of interests towards popular items and then it is showed that there is a utility discrepancy between users who like popular items versus the users who like niche less popular items. The latter group experiences lower quality recommendations. \cite{abdollahpouri2020addressing} also presents a re-ranking approach based on idea of calibration in Steck \cite{steck2018calibrated} to improve the fairness for these consumer groups.
    
    While using algorithmic interventions to reduce disparate effectiveness, we should cautiously consider the difference between consumer side fairness and provider side fairness. In consumer fairness, utility is not a rivalrous good; therefore, increasing on group's utility doesn't corrupt the utility for the other users. However, in provider fairness, the recommendation spots are limited and providers compete to get those slot. 
    
    \subsection{Beyond Accuracy Fairness}
    
    Another way of assessing a system to see whether its users are experiencing discrimination or harm is to look at their experiences from other perspectives besides utility, such as stereotyping. Ali et al.\cite{Ali2019Facebook} studied the implications of Facebook's ad delivery process. They observed significant skew in ad delivery along gender and racial lines for employment and housing opportunity ads, despite neutral ad targeting parameters. They demonstrate that unknown market mechanisms in combination with relevance optimization results in this disparate ad distribution. Nasr and Tschantz \cite{nasr2020bidding} propose a bidding strategy to ensure a fair ad distribution. 
    Kamishima and Akaho \cite{kamishima2017considerations, kamishima2018recommendation} propose to use a probabilistic test of independence to see whether the results are independent of the protected group attribute. They incorporate this idea to the loss function of matrix factorization to obtain results that are uncorrelated with the protected attribute. This method can be used when the protected users should not be recommended certain types of items. Beutel et al.\cite{beutel2017data} approach learns fair representations (such as user embeddings or item embeddings in recommender systems) in an adverserial setting that has been set up to minimize the ability to predict users' sensitive attributes. By using this method in recommender systems, the stereotype effects can be prevented.
    % \todo[inline]{example?}
    
    \subsection{Complex Scenarios}
    Most of the work on consumer fairness to date has focused on simpler settings, such as considering fairness as a concept that entails only a single protected attribute or a single dimension of the sensitive attributes. And even when multiple sensitive attributes are considered, such as gender and age and race, etc., they are considered separately. In reality, a series of protected attributes might need to be considered simultaneously as determined by laws or organizational requirements. Crenshaw \cite{crenshaw1989demarginalizing} explains about the complexities of multiple protected groups under the framework of intersectionality. In fair machine learning, Kearns et al. \cite{kearns2019empirical} defines this concept as rich subgroup fairness and in recommender systems this issue has been studied for providers by Sonboli et al. 2020 \cite{sonboli2020opportunistic}  (explained in Chapter \ref{ch:fairness_postproc} in Section \ref{sec:ofair}). There are no other existing work to date, that addresses this problem in depth. Another simplification in modeling consumer-side fairness is to assume fairness concerns are binary (protected vs unprotected) rather than multiple attributes that might include continuous features. There is not any work in recommender systems that considers this nuances for fairness concepts.


\section{Provider Fairness}
\todo[inline]{missing}

\section{Dynamic Fairness}

    It is important to consider fairness in recommender systems as a dynamic property because they operate in a dynamic environment. A system continuously makes new decisions based on recent changes in the system, such as gaining or losing consumers, gaining or losing (item) providers, and the different items they provide based on the current needs of the market, seasonal changes, etc. Some of these dynamic changes can introduce biases into a system that might have been ``de-biased'' recently or they can even reinforce the existing biases. 
    
    \textbf{Positive Feedback Loops} are among the most destructive dynamic changes that happen in recommender systems \cite{o2016weapons} that are caused by presentation bias. This type of bias occurs because users are more likely to interact with items that the system presents to them. 
    
    The first issue here is the item selection by the recommender system here and whether it will contribute to unfairness to any of the stakeholders. We have addressed how to mitigate this issue in Chapter \ref{ch:fairness}.The second issue is that presentation bias, can lead to a form of the positive feedback loop, in which presented items gain more popularity since they are more likely to be interacted with. This leads to greater bias towards presenting the items when the popular items are promoted more at the cost of other items. Presentation bias and the created feedback loop not only magnifies the initial differences between items' popularity but also it makes it hard for new providers to attract the attention of users to their products/items in a system with this type of bias.
    %  in the pre-processing section \todo[inline]{preprocessing section}. 
    
    For example, in the microlending case, if the system doesn't recommend loans from a specific geographical region, because on average, the requested loans from this region are risky (their borrowers are less likely to repay the loan), not only the current good borrowers (the borrowers who are more likely to return the loan) from that region are affected, but also the future good borrowers. This positive feedback loop reinforces over time until that region is completely ignored by the system.
    
    Chaney et al. \cite{Chaney2018Homo} show that in an iterative environment, recommender systems increase the inequity in item exposure and homogenize the recommendation lists across different users, which is undesirable. \cite{pmlr-v80-hashimoto18a} shows in a systems that users engage with it iteratively, machine learning models that are trained on average loss, suffer from representation disparity (presentation bias) which over time these systems lose their minority groups. To mitigate this bias, they develop an approach based on distributionally robust optimization (DRO) that achieves a more balanced performance between the dominant subgroups and minority ones. Zhang et al. \cite{NEURIPS2019_7690dd4d} study this idea in a sequential framework and show that representation disparity gets worse under the natural user dynamics model, resulting in some groups diminishing entirely from the sample pool in the long run. To mitigate feedback loops in the context of predictive policing, Ensign et al. \cite{pmlr-v81-ensign18a} develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system so the feedback loop does not occur, leading to better decision-making systems.To detect and evaluate dynamic biases in systems, It is necessary to have dynamic evaluation methodologies as well.
    
    \textbf{Dynamic Evaluation} methodologies, therefore, are required to evaluate such biases in a dynamic system. Here, instead of using batch training and then batch testing, we need to incorporate a cycle that starts with user arrival, training the model, recommendation generation, and then collection of user feedback on those recommendations and continues with periodically re-training the system and generating new sets of recommendations at every iteration.
    
    This evaluation methodology has become common in recommendation approaches that use reinforcement learning where the environment is dynamic \cite{Lihong2010bandit,Zheng2018DRN}. Evaluation of this cycle can also be simulated in offline settings. To appropriately evaluate the fairness properties of recommender systems, in practice, we might need to look at unfairness in a particular time interval. And then we might have to consider the evolution of unfairness mitigation and/or aggravation through multiple evaluation cycle.
    
    I have made use of this evaluation methodology in reranking \cite{sonboli2020dynm}. I propose an adaptive recommendation approach to address the problem of multidimensional fairness using the probabilistic social choice to control feedback loops and adjust the system over time. I compute deviations from fairness in a particular time window and then compensate it over the next batch of generated recommendations by adjusting the fairness objective. For more details please refer to Chapter \ref{ch:fairness_postproc}. Biega et al. \cite{biega2018equity} use time in their re-ranking as well. They consider past rankings so that the ranking at a particular time $t$ improves the aggregate fairness of the system through time $t$.
    
    
    In the following chapter, I will introduce baseline recommendations, datasets, the approaches we adopted to define the protected group(s), the unprotected group(s), and all the details of the experimental design to increase reproducibility.